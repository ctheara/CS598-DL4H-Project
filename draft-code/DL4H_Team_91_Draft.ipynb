{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Important Instructions\n",
        "\n",
        "####To get the Dataset mounted on Google Drive:  \n",
        "\n",
        "*  Go to MIMIC IV Website: https://physionet.org/content/mimiciv/2.2\n",
        "*  Download the dataset to your drive\n",
        "*  Validate the dataset is present under -> MyDrive/mimiciv/2.2/hosp.\n",
        "    Example:\n",
        "    *   /content/drive/MyDrive/mimiciv/2.2/hosp/admissions.csv.gz\n",
        "    *   /content/drive/MyDrive/mimiciv/2.2/hosp/diagnoses_icd.csv.gz\n",
        "\n",
        "####Link to the Project draft Google Colab Notebook (.ipynb):\n",
        "https://colab.research.google.com/drive/1RMl4T3FsAQaJDColj0xPma8t3Xwk0c3z?authuser=1#scrollTo=-Ua2cM28fPO4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Ua2cM28fPO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports Modules"
      ],
      "metadata": {
        "id": "sIR1ETz8aLQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Required installations\n",
        "#!pip install transformers[torch]\n",
        "!pip install --upgrade accelerate>=0.21.0\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "ZHRWuuAmZrKr",
        "outputId": "154175eb-657f-445e-b860-d31dbfaeab42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding all modules to import for the project.\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import logging\n",
        "import os\n",
        "from typing import Callable, Dict, List, Optional, Tuple\n",
        "import csv\n",
        "import json, time\n",
        "from collections import defaultdict\n",
        "from itertools import combinations, islice\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "torch.__version__\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
        "\n",
        "from transformers.data.data_collator import DataCollator\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, EvalPrediction, PredictionOutput, TrainOutput\n",
        "from transformers.training_args import TrainingArguments\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.models.bart.configuration_bart import BartConfig\n",
        "from transformers import BertTokenizer, BartTokenizer\n",
        "# from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_callable\n",
        "\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    BertTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    HfArgumentParser,\n",
        "    LineByLineTextDataset,\n",
        "    PreTrainedTokenizer,\n",
        "    TextDataset,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from transformers import Trainer\n",
        "#from dataset import DataCollatorForICDBERT, DataCollatorForICDBERTFINALPRED, DataCollatorForICDBART\n",
        "#from icdmodelbart import ICDBartForPreTraining"
      ],
      "metadata": {
        "id": "6Bot1-lfandR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Pyhealth installation and related module imports for future enhacements\n",
        "\n",
        "# !pip install pyhealth\n",
        "# # PyHealth modules\n",
        "# from pyhealth.data import *\n",
        "# from pyhealth.datasets import *\n",
        "# from pyhealth.tasks import *\n",
        "# from pyhealth.models import *\n",
        "# from pyhealth.trainer import *\n",
        "# from pyhealth.medcode import *\n",
        "# from pyhealth.tokenizer import *"
      ],
      "metadata": {
        "id": "tp8_VqrnNonE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v51zRh9WK3EO",
        "outputId": "61e96ec9-c3eb-4a3d-d8b5-37fb08d9cbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the File listing\n",
        "\n",
        "!ls -lr /content/drive/MyDrive/mimiciv/2.2/*"
      ],
      "metadata": {
        "id": "dZHMjFu6WwqF",
        "outputId": "ace24363-9138-4462-cb08-45bf4e15ef07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw------- 1 root root  2884 Jan  6  2023 /content/drive/MyDrive/mimiciv/2.2/SHA256SUMS.txt\n",
            "-rw------- 1 root root  2518 Jan  6  2023 /content/drive/MyDrive/mimiciv/2.2/LICENSE.txt\n",
            "-rw------- 1 root root   789 Mar 29 00:31 /content/drive/MyDrive/mimiciv/2.2/index.html\n",
            "-rw------- 1 root root 13332 Jan  5  2023 /content/drive/MyDrive/mimiciv/2.2/CHANGELOG.txt\n",
            "\n",
            "/content/drive/MyDrive/mimiciv/2.2/icu:\n",
            "total 3077969\n",
            "-rw------- 1 root root   20717852 Jan  5  2023 procedureevents.csv.gz\n",
            "-rw------- 1 root root   38747895 Jan  5  2023 outputevents.csv.gz\n",
            "-rw------- 1 root root  324218488 Jan  5  2023 inputevents.csv.gz\n",
            "-rw------- 1 root root  251962313 Jan  5  2023 ingredientevents.csv.gz\n",
            "-rw------- 1 root root       1336 Mar 29 00:31 index.html\n",
            "-rw------- 1 root root    2614571 Jan  5  2023 icustays.csv.gz\n",
            "-rw------- 1 root root      57476 Jan  5  2023 d_items.csv.gz\n",
            "-rw------- 1 root root   45721062 Jan  5  2023 datetimeevents.csv.gz\n",
            "-rw------- 1 root root 2467761053 Jan  5  2023 chartevents.csv.gz\n",
            "-rw------- 1 root root      35893 Jan  5  2023 caregiver.csv.gz\n",
            "\n",
            "/content/drive/MyDrive/mimiciv/2.2/hosp:\n",
            "total 4429839\n",
            "-rw------- 1 root root   36158338 Jan  5  2023 transfers.csv.gz\n",
            "-rw------- 1 root root    6781247 Jan  5  2023 services.csv.gz\n",
            "-rw------- 1 root root     122507 Jan  5  2023 provider.csv.gz\n",
            "-rw------- 1 root root    6027067 Jan  5  2023 procedures_icd.csv.gz\n",
            "-rw------- 1 root root  458817415 Jan  5  2023 prescriptions.csv.gz\n",
            "-rw------- 1 root root   25477219 Jan  5  2023 poe_detail.csv.gz\n",
            "-rw------- 1 root root  498505135 Jan  5  2023 poe.csv.gz\n",
            "-rw------- 1 root root  398753125 Jan  5  2023 pharmacy.csv.gz\n",
            "-rw------- 1 root root    2312631 Jan  5  2023 patients.csv.gz\n",
            "-rw------- 1 root root   36124944 Jan  5  2023 omr.csv.gz\n",
            "-rw------- 1 root root   96698496 Jan  5  2023 microbiologyevents.csv.gz\n",
            "-rw------- 1 root root 1939088924 Jan  5  2023 labevents.csv.gz\n",
            "-rw------- 1 root root       2907 Mar 29 00:31 index.html\n",
            "-rw------- 1 root root    1767138 Jan  5  2023 hcpcsevents.csv.gz\n",
            "-rw------- 1 root root  471096030 Jan  5  2023 emar_detail.csv.gz\n",
            "-rw------- 1 root root  508524623 Jan  5  2023 emar.csv.gz\n",
            "-rw------- 1 root root    7426955 Jan  5  2023 drgcodes.csv.gz\n",
            "-rw------- 1 root root      12900 Jan  5  2023 d_labitems.csv.gz\n",
            "-rw------- 1 root root     578517 Jan  5  2023 d_icd_procedures.csv.gz\n",
            "-rw------- 1 root root     859438 Jan  5  2023 d_icd_diagnoses.csv.gz\n",
            "-rw------- 1 root root   25070720 Jan  5  2023 diagnoses_icd.csv.gz\n",
            "-rw------- 1 root root     427468 Jan  5  2023 d_hcpcs.csv.gz\n",
            "-rw------- 1 root root   15516088 Jan  5  2023 admissions.csv.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "  Longitudinal Electronic Health Records (EHRs) successfully used for clinical disease and outcome prediction using Deep Learning models.\n",
        "  State-of-the-art (SOTA) models outperform traditional ML models by using pretrain-finetune methods in EHR-based predictive modeling. However, their pre-training objectives are limited in predicting fraction of ICD codes within each visit. In real life scenarios, patients have multiple diseases which can be correlated and can contribute to disease progression and change in outcome.\n",
        "  Additionally, generalizing the same model on out-of-domain data in different medical settings with limited computing resources is a major challenge today.\n",
        "\n",
        "\n",
        "*   Paper explanation\n",
        "\n",
        "  The paper proposes TransformEHR, which is a generative encoder-decoder model with a transformer that is pre-trained using a new strategy, to predict the complete set of diseases and outcomes of patients at a future visit from previous visits. The model is generalizable and can be finetuned for various clinical prediction tasks with limited data.\n",
        "\n",
        "  TransformEHR uses encoder-decoder transformer architecture. The encoder processes the input embeddings and generates a set of hidden representations. The model performs cross-attention over hidden representations from the encoder and assigns an attention weight for each representation. The decoder generates ICD codes following the sequential order of code priority within a visit. It includes the date of each visit as a feature to integrate temporal information. The model uses 3 unique components compared to state-of-the-art models (BERT - Bidirectional Encoder Representations from Transformers) i.e. Visit masking, Encoder-decoder architecture and time embedding.\n",
        "\n",
        "  As per author, during the pretraining with a larger set of longitudinal EHR data, TransformEHR model learned the probability distribution of ICD codes through correlation of cross attention. Later It was fine-tuned to the predictions of a single disease or outcome.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "Hypothesis1: Whether using the TransformEHR model can effectively predict the complete set of diseases and outcomes of a patient from past visits (i.e. Disease or outcome agnostic prediction (DOAP) task). The result will be compared to the state-of-the-art model (BERT - Bidirectional Encoder Representations from Transformers) that is usually trained to predict a fraction of ICD codes within each visit. The model will use a transformer architecture and seek to outperform bidirectional encode-only models.\n",
        "\n",
        "Hypothesis2: The other hypothesis could be that the TransformEHR model can perform better for single disease outcomes with pre-training than without pre-training. After carefully reviewing the large data requirement of the VHA dataset for pretraining, we realized that testing this hypothesis may not be feasible with limited computational resources.\n",
        "\n",
        "Abalations planned:\n",
        "1.\tTo evaluate the effectiveness of 3 of the unique components of the TransformEHR model, which are visit masking, encoder-decoder architecture, and time embedding.\n",
        "2.\tTo assess the performance of an Encoder-only architecture model compared to an encoder-decoder architecture.\n",
        "3.\tImpact of the inclusion and exclusion of certain temporal features such as date of each visit.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are planning to follow below steps for this implementation:\n",
        "\n",
        "* Data Load - Extract below data files from MIMIC-IV dataset https://physionet.org/content/mimiciv/2.2 :\n",
        "\n",
        "    1.   admissions.csv.gz\n",
        "    2.   diagnoses_icd.csv.gz\n",
        "\n",
        "* Preprocess MIMIC-IV dataset files:\n",
        "\n",
        "    * Create subset dataset by extracting ICD10 version records to align with the cohorts from Pretrained model.\n",
        "\n",
        "    * For Project Draft purpose - create subset of data (100 Patients and related records from admissions and diagnosis_icd files)\n",
        "\n",
        "    * Map the ICD codes to CUI codes [Concepts to concept Unique Identifiers (CUIs) from the United Medical Language System (UMLS)]\n",
        "\n",
        "* Create Test and Train datasets  \n",
        "\n",
        "* Model Architecture:\n",
        "    *   Create Positional Embedding\n",
        "    *   Multi-head attention mechanism\n",
        "    *   Encoder Layer\n",
        "    *   Decoder Layer\n",
        "* Model Initialization\n",
        "* Model Training\n",
        "* Model Evaluation\n",
        "* Results\n",
        "\n"
      ],
      "metadata": {
        "id": "q6K5vQ1I9k0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MIMIC-IV dataset comprises data from intensive care unit patients admitted to the Beth Israel Deaconess Medical Center in Boston, Massachusetts. Although the dataset spans from 2008 to 2019, the implementation of ICD-10CM began in October 2015. As per authors implementation plan, to align with the cohorts from the Veterans Health Administration (VHA) dataset pretrained model, only patients with ICD-10CM records will be selected.\n",
        "\n",
        "\n",
        "Data Load - Extract below data files from MIMIC-IV dataset https://physionet.org/content/mimiciv/2.2 :\n",
        "\n",
        "1.   admissions.csv.gz\n",
        "2.   diagnoses_icd.csv.gz\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "3Xezd9DsMIqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read diagnoses_icd.csv.gz\n",
        "diagnoses_df = pd.read_csv('/content/drive/MyDrive/mimiciv/2.2/hosp/diagnoses_icd.csv.gz',\n",
        "                           nrows=None,\n",
        "                           compression='gzip',\n",
        "                           dtype={'subject_id': str, 'hadm_id': str, 'icd_code': str, 'icd_version': str},\n",
        "#                           error_bad_lines=False)\n",
        "                           on_bad_lines = 'skip')\n",
        "print(f'Number of Rows and Columns in Diagnoses_icd file: {diagnoses_df.shape}')\n",
        "#print(diagnoses_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qalfqs3UMGUa",
        "outputId": "9ec3bf54-751d-4802-dfb8-0647a05976b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Rows and Columns in Diagnoses_icd file: (4756326, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read admissions.csv.gz file\n",
        "admissions_df = pd.read_csv('/content/drive/MyDrive/mimiciv/2.2/hosp/admissions.csv.gz',\n",
        "                            nrows=None,\n",
        "                            compression='gzip',\n",
        "                            dtype={'subject_id': str, 'hadm_id': str},\n",
        "#                            error_bad_lines=False)\n",
        "                           on_bad_lines = 'skip')\n",
        "print(f'Number of Rows and Columns in Admissions file: {admissions_df.shape}')\n",
        "#print(admissions_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr_kcl3cmiKo",
        "outputId": "d7b11a03-e739-40f0-b1c0-4b45a1a9b123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Rows and Columns in Admissions file: (431231, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocess Data"
      ],
      "metadata": {
        "id": "vrVfY8loAWy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Create subset dataset by extracting ICD10 version records to align with the cohorts from Pretrained model.\n",
        "\n",
        "*   For Project Draft purpose - create subset of data (100 Patients and related records from admissions and diagnosis_icd files)\n",
        "\n",
        "*   Map the ICD codes to CUI codes [Concepts to concept Unique Identifiers (CUIs) from the United Medical Language System (UMLS)]\n",
        "\n"
      ],
      "metadata": {
        "id": "y5peRp8zMz-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter icd_version=10\n",
        "diagnoses_df = diagnoses_df[diagnoses_df['icd_version'] == '10']\n",
        "print(f'Number of Rows and Columns in Diagnoses_icd_10 file: {diagnoses_df.shape}')\n",
        "\n",
        "# Select 100 unique subject_id from the filtered diagnoses file\n",
        "selected_subject_ids = diagnoses_df['subject_id'].unique()[:100]\n",
        "print(f'Selected 100 Patients from Diagnoses_icd_10 file: {selected_subject_ids}')\n",
        "\n",
        "\n",
        "# Filter both the Diagnoses and admissions to include only the selected 100 subject_id\n",
        "data_df = diagnoses_df[diagnoses_df['subject_id'].isin(selected_subject_ids)]\n",
        "\n",
        "print(f'Number of Rows and Columns in Subset dataset Diagnoses_icd_10 file: {data_df.shape}')\n",
        "admissions_data_df = admissions_df[admissions_df['subject_id'].isin(selected_subject_ids)]\n",
        "print(f'Number of Rows and Columns in Subset dataset Admissions file: {admissions_data_df.shape}')\n",
        "#print(len(data_df))\n",
        "#print(len(admissions_data_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gja0bMFYVwd",
        "outputId": "9afc42a5-dd44-4a28-ce38-f0b232041389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Rows and Columns in Diagnoses_icd_10 file: (1989449, 5)\n",
            "Selected 100 Patients from Diagnoses_icd_10 file: ['10000084' '10000117' '10000980' '10001401' '10001667' '10001843'\n",
            " '10001884' '10001919' '10002013' '10002131' '10002221' '10002266'\n",
            " '10002315' '10002348' '10002428' '10002430' '10002443' '10002495'\n",
            " '10002528' '10002545' '10002755' '10002800' '10002807' '10002869'\n",
            " '10002930' '10002976' '10003019' '10003299' '10003372' '10003385'\n",
            " '10003400' '10003412' '10003502' '10003637' '10003731' '10003757'\n",
            " '10004113' '10004296' '10004322' '10004457' '10004606' '10004719'\n",
            " '10004720' '10004764' '10005001' '10005123' '10005236' '10005308'\n",
            " '10005606' '10005749' '10005817' '10005858' '10005866' '10005909'\n",
            " '10006029' '10006269' '10006431' '10006457' '10006513' '10006630'\n",
            " '10006640' '10006825' '10007058' '10007134' '10007232' '10007266'\n",
            " '10007818' '10007920' '10008077' '10008245' '10008287' '10008647'\n",
            " '10008742' '10008816' '10008819' '10009129' '10010038' '10010058'\n",
            " '10010150' '10010231' '10010393' '10010471' '10010655' '10010663'\n",
            " '10010718' '10010848' '10010867' '10010993' '10010997' '10011189'\n",
            " '10011259' '10011279' '10011365' '10011427' '10011449' '10011607'\n",
            " '10011668' '10011912' '10011938' '10012206']\n",
            "Number of Rows and Columns in Subset dataset Diagnoses_icd_10 file: (2741, 5)\n",
            "Number of Rows and Columns in Subset dataset Admissions file: (352, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the Max and Min of discharge date in admissions subset\n",
        "visitid2dischargedate = {}\n",
        "for ind, row in admissions_data_df.iterrows():\n",
        "    visitid2dischargedate[row['hadm_id']] = row['dischtime'][0:10]\n",
        "\n",
        "print(min(visitid2dischargedate.values()))\n",
        "print(max(visitid2dischargedate.values()))\n",
        "print(visitid2dischargedate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWtnpUoyAeGt",
        "outputId": "4b071b2b-1945-4f3d-aeb1-9badbef79035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2112-12-10\n",
            "2204-07-25\n",
            "{'23052089': '2160-11-25', '29888819': '2160-12-28', '22927623': '2181-11-15', '27988844': '2183-09-21', '20897796': '2193-08-17', '24947999': '2190-11-08', '25242409': '2191-04-11', '25911675': '2191-05-24', '26913865': '2189-07-03', '29654838': '2188-01-05', '29659838': '2191-07-19', '21544441': '2131-06-15', '24818636': '2131-08-04', '26840593': '2131-07-02', '27012892': '2133-07-13', '27060146': '2131-10-05', '28058085': '2131-11-15', '22672901': '2173-08-24', '21728396': '2131-11-11', '21192799': '2130-10-06', '21268656': '2125-10-20', '21577720': '2125-12-27', '22532141': '2130-10-14', '23594368': '2125-12-03', '24325811': '2126-11-04', '24746267': '2130-12-30', '24962904': '2130-12-08', '25758848': '2128-07-17', '26170293': '2130-04-19', '26184834': '2131-01-20', '26202981': '2130-08-23', '26679629': '2125-10-27', '26812645': '2127-07-25', '27016754': '2130-06-24', '27507515': '2130-12-24', '27765344': '2127-12-12', '28475784': '2130-10-22', '28664981': '2130-11-30', '28669374': '2130-11-20', '29675586': '2130-04-09', '29678536': '2130-10-12', '29897682': '2124-04-21', '21763296': '2165-11-26', '21975601': '2159-12-17', '23581541': '2160-05-23', '23745275': '2157-11-01', '24760295': '2160-07-12', '24848509': '2162-07-09', '25442395': '2166-04-19', '25715803': '2156-11-05', '27574273': '2164-03-19', '28420602': '2161-02-09', '28603984': '2156-07-02', '28629319': '2167-07-05', '22798184': '2123-06-26', '24065018': '2128-03-19', '27411540': '2123-04-03', '20195471': '2203-06-16', '20237862': '2204-07-06', '21008195': '2200-10-01', '21729093': '2204-06-30', '29399017': '2204-07-25', '24160398': '2131-03-06', '22798410': '2161-03-24', '22725460': '2112-12-10', '20321825': '2156-05-03', '23473524': '2156-05-22', '25797028': '2155-07-15', '26549334': '2160-07-16', '28295257': '2160-04-18', '28662225': '2156-04-29', '28676446': '2157-07-18', '24513842': '2125-09-30', '24648311': '2129-05-02', '26295318': '2129-06-24', '27218502': '2125-06-25', '21329021': '2183-10-20', '24982426': '2141-05-29', '23193578': '2168-12-20', '28605730': '2170-03-18', '21824331': '2158-09-10', '22757696': '2141-12-14', '20798638': '2164-07-14', '21095886': '2164-08-04', '22634923': '2164-02-25', '26199514': '2165-01-28', '28464737': '2152-03-31', '22289895': '2136-04-06', '23076388': '2136-07-27', '27026861': '2134-01-30', '20282368': '2201-03-26', '20846853': '2201-02-13', '22380825': '2193-08-05', '22733922': '2198-05-04', '23688993': '2193-08-11', '23720373': '2199-02-19', '25282382': '2197-04-17', '25696644': '2196-04-17', '25922998': '2198-04-22', '28301173': '2197-04-15', '28477649': '2197-04-08', '28697806': '2200-06-05', '21640325': '2145-03-18', '24808189': '2153-08-14', '27179825': '2145-03-05', '20030125': '2174-12-27', '20277210': '2175-11-15', '20962108': '2176-01-14', '21213148': '2175-12-02', '21223482': '2175-11-02', '21616816': '2174-09-21', '22774359': '2175-10-17', '23693618': '2171-05-05', '24646702': '2174-10-25', '25179393': '2175-12-11', '25573783': '2173-12-09', '27683372': '2175-09-06', '28836362': '2181-03-04', '20920687': '2183-06-19', '20940957': '2183-07-01', '21404960': '2179-07-02', '21476780': '2183-02-28', '21743184': '2173-05-13', '22087674': '2173-11-21', '27373340': '2183-08-01', '28891311': '2178-12-11', '29323205': '2181-10-23', '22448043': '2185-07-08', '23040642': '2131-09-26', '20214994': '2137-03-19', '22390287': '2137-02-18', '23559586': '2137-09-02', '26090619': '2134-06-07', '26467376': '2136-12-15', '27296885': '2137-01-03', '29483621': '2136-11-12', '28884815': '2173-01-20', '20459702': '2166-02-19', '21671572': '2163-02-06', '22491625': '2161-07-01', '23352834': '2164-02-27', '26475031': '2166-02-21', '29011269': '2169-08-28', '29391916': '2165-12-26', '29460260': '2163-04-13', '22082422': '2146-02-19', '22394931': '2145-11-24', '23487925': '2146-01-26', '26115941': '2145-01-06', '28484061': '2149-05-20', '23646008': '2146-11-19', '24817944': '2152-04-14', '26829618': '2142-09-15', '29879900': '2173-03-22', '21736479': '2168-11-08', '20356134': '2135-02-12', '21479295': '2135-01-03', '22602599': '2134-10-02', '28755331': '2131-01-26', '21039249': '2140-09-18', '21216581': '2143-03-10', '23251352': '2141-12-21', '25559382': '2148-09-15', '28108313': '2147-12-21', '28723315': '2141-08-13', '23517634': '2159-03-22', '28691361': '2159-09-22', '28731738': '2159-04-11', '29242151': '2159-03-06', '21197153': '2183-09-03', '22081550': '2186-11-17', '24817563': '2168-04-16', '20438270': '2160-06-22', '23954664': '2165-06-18', '25115899': '2164-10-14', '20470681': '2129-08-19', '24274954': '2129-07-05', '23384508': '2180-07-03', '25656545': '2177-11-30', '20445854': '2178-04-20', '22751751': '2144-03-08', '29646384': '2143-12-16', '20010003': '2140-09-29', '24015009': '2145-09-14', '24979738': '2144-01-31', '20626031': '2132-12-20', '28661809': '2135-01-19', '22585238': '2172-07-20', '24193796': '2177-02-11', '28426363': '2168-09-12', '28576445': '2167-11-29', '29352282': '2172-08-17', '29569314': '2177-07-21', '20364112': '2149-10-25', '21636229': '2149-09-26', '22589518': '2149-02-14', '23514107': '2149-06-25', '26134779': '2149-09-19', '26158160': '2146-06-09', '27167814': '2148-03-21', '20199380': '2144-11-02', '20850584': '2170-07-12', '25426298': '2170-08-11', '27104518': '2169-10-05', '29107716': '2169-08-22', '27357430': '2124-07-05', '24638489': '2129-01-30', '25086012': '2129-02-06', '25589898': '2128-01-16', '27715811': '2128-03-07', '28484351': '2128-06-12', '28771670': '2128-03-30', '23963539': '2152-12-29', '24934308': '2151-11-08', '27072986': '2154-01-29', '27894366': '2147-12-17', '28504108': '2125-05-07', '29846618': '2127-03-28', '27341635': '2177-03-01', '29151310': '2173-12-19', '24019717': '2150-08-03', '24055420': '2155-09-23', '24767236': '2155-09-29', '26690696': '2157-05-28', '29669544': '2150-08-10', '22954658': '2167-11-11', '29356606': '2140-05-24', '28553812': '2120-06-07', '27967951': '2189-01-20', '22987108': '2146-07-12', '20423193': '2139-05-07', '20640664': '2132-07-04', '20862473': '2141-08-08', '22218665': '2138-12-10', '22785199': '2138-12-27', '23867410': '2139-01-01', '23967347': '2135-11-29', '24504634': '2132-06-18', '26254531': '2132-08-21', '26391915': '2138-12-20', '26693451': '2136-08-30', '28758019': '2138-11-30', '29607451': '2135-12-02', '22125576': '2189-03-01', '27668147': '2191-03-07', '26674944': '2174-04-21', '22168393': '2145-10-02', '25482427': '2118-04-01', '27774082': '2173-06-26', '21347084': '2129-08-23', '21413948': '2126-05-29', '21608567': '2128-12-19', '22267961': '2129-04-04', '22359395': '2125-11-28', '22801915': '2132-03-15', '24039430': '2129-07-09', '24292344': '2126-05-22', '26081626': '2130-08-21', '26291469': '2129-05-17', '26783306': '2129-05-21', '29083697': '2129-04-05', '29144873': '2126-06-05', '29616482': '2130-11-28', '26880688': '2169-08-25', '21618536': '2183-10-23', '28266696': '2167-03-18', '21955805': '2147-01-06', '25502652': '2140-01-29', '25917765': '2139-06-29', '26359957': '2147-11-19', '28963312': '2145-10-03', '20546258': '2121-04-05', '20687038': '2118-03-09', '21586397': '2117-12-23', '23083980': '2118-02-07', '23835132': '2118-04-07', '24995642': '2118-02-26', '25499227': '2117-12-05', '27998273': '2118-05-09', '28743978': '2118-01-08', '29368887': '2118-01-20', '23840407': '2136-07-07', '25242586': '2136-11-13', '27377841': '2136-07-02', '28846987': '2134-12-12', '21322534': '2155-05-10', '29842315': '2155-12-07', '20421864': '2167-09-01', '22692881': '2167-03-21', '24593549': '2167-05-11', '24806124': '2167-07-14', '24811327': '2167-08-29', '26376973': '2168-03-29', '27035847': '2167-03-08', '29222372': '2167-07-27', '22209635': '2146-10-13', '29947355': '2169-01-27', '25916492': '2169-01-28', '22196214': '2148-03-13', '22429197': '2148-01-11', '22950920': '2148-01-30', '28481035': '2116-05-15', '20783870': '2139-05-02', '23456305': '2188-03-25', '27462671': '2189-07-04', '29477116': '2188-02-26', '23789480': '2161-05-05', '24004760': '2161-07-03', '24468341': '2161-05-20', '29284526': '2161-07-08', '29504188': '2179-05-03', '25048984': '2164-09-11', '26712576': '2166-02-20', '26722872': '2166-01-28', '26948481': '2157-07-22', '27047580': '2164-09-28', '20083129': '2136-03-17', '20219031': '2136-04-08', '22216667': '2136-02-12', '28855838': '2135-12-26', '27619916': '2135-12-01', '23426764': '2184-04-30', '24673789': '2182-01-12', '27869300': '2185-01-27', '28161429': '2183-06-30', '28165131': '2185-03-19', '28616773': '2184-04-25', '29715332': '2183-08-14', '22181970': '2131-06-24', '22868305': '2133-01-26', '23256585': '2134-01-17', '23507785': '2136-12-08', '24061001': '2141-04-20', '24491387': '2137-01-02', '25142813': '2133-12-14', '25435238': '2134-05-18', '28654569': '2131-07-27', '29565095': '2131-07-11', '29825872': '2133-06-04', '20984043': '2176-10-21', '21264695': '2171-07-10', '22567237': '2179-09-25', '23959856': '2170-01-27', '28943379': '2170-03-06', '29105198': '2177-10-05', '22624746': '2128-01-11', '23501236': '2128-02-26', '24278970': '2128-12-23', '24772774': '2132-01-30', '24943792': '2121-06-23', '29339128': '2127-05-14', '23961896': '2127-07-14'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Patient dictionary\n",
        "#Code reference: https://github.com/whaleloops/TransformEHR/blob/main/preprocess.py\n",
        "\n",
        "patients = defaultdict(lambda: defaultdict(list))\n",
        "print(\"Number of rows:\" , data_df.shape[0])\n",
        "for ind, row in data_df.iterrows():\n",
        "    hadm_id = row['hadm_id']\n",
        "    scrssn = row['subject_id']\n",
        "    visit_date = visitid2dischargedate[hadm_id]\n",
        "    patients[scrssn][visit_date].append(row['icd_version'] +'-'+ row['icd_code'])\n",
        "\n",
        "num_icd_pat = defaultdict(int)\n",
        "for k,v in patients.items():\n",
        "    for kv, vv in v.items():\n",
        "        for icdcode in vv:\n",
        "            if icdcode.startswith(\"10-\"):\n",
        "                num_icd_pat[k] += 1\n",
        "                break\n",
        "\n",
        "#print(len(patients))\n",
        "#print(len(num_icd_pat))\n",
        "num_pos = 0\n",
        "for k,v in num_icd_pat.items():\n",
        "    if v > 1:\n",
        "        num_pos += 1\n",
        "#print(num_pos)\n",
        "print(f'num_icd_pat dictionary: {num_icd_pat}')\n",
        "#print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5paESXKzBFcq",
        "outputId": "86e2bc62-f678-4980-d570-ff0f3bc71bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 2741\n",
            "num_icd_pat dictionary: defaultdict(<class 'int'>, {'10000084': 2, '10000117': 2, '10000980': 3, '10001401': 6, '10001667': 1, '10001843': 1, '10001884': 12, '10001919': 1, '10002013': 4, '10002131': 1, '10002221': 4, '10002266': 1, '10002315': 1, '10002348': 1, '10002428': 2, '10002430': 4, '10002443': 1, '10002495': 1, '10002528': 2, '10002545': 1, '10002755': 1, '10002800': 4, '10002807': 1, '10002869': 2, '10002930': 6, '10002976': 1, '10003019': 1, '10003299': 5, '10003372': 1, '10003385': 1, '10003400': 1, '10003412': 1, '10003502': 1, '10003637': 4, '10003731': 1, '10003757': 1, '10004113': 1, '10004296': 1, '10004322': 3, '10004457': 2, '10004606': 4, '10004719': 1, '10004720': 1, '10004764': 1, '10005001': 2, '10005123': 2, '10005236': 1, '10005308': 1, '10005606': 2, '10005749': 1, '10005817': 1, '10005858': 2, '10005866': 6, '10005909': 1, '10006029': 4, '10006269': 1, '10006431': 6, '10006457': 3, '10006513': 2, '10006630': 1, '10006640': 1, '10006825': 1, '10007058': 1, '10007134': 1, '10007232': 1, '10007266': 1, '10007818': 1, '10007920': 1, '10008077': 2, '10008245': 1, '10008287': 1, '10008647': 1, '10008742': 1, '10008816': 10, '10008819': 1, '10009129': 1, '10010038': 1, '10010058': 3, '10010150': 1, '10010231': 9, '10010393': 3, '10010471': 2, '10010655': 8, '10010663': 1, '10010718': 1, '10010848': 1, '10010867': 3, '10010993': 1, '10010997': 1, '10011189': 3, '10011259': 2, '10011279': 1, '10011365': 2, '10011427': 4, '10011449': 1, '10011607': 2, '10011668': 1, '10011912': 3, '10011938': 2, '10012206': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Map the ICD codes to CUI codes [Concepts to concept Unique Identifiers (CUIs) from the United Medical Language System (UMLS)]\n",
        "#Code reference: https://github.com/whaleloops/TransformEHR/blob/main/preprocess.py\n",
        "\n",
        "def icd2cui(patients, logging_step=50000):\n",
        "    dictionary = defaultdict(int)\n",
        "    # cuis_li = []\n",
        "    cuis_di = {}\n",
        "    date_di = {}\n",
        "    num_idx = 0\n",
        "    for pssn,v in patients.items():\n",
        "        num_idx += 1\n",
        "        if num_idx%logging_step == 0:\n",
        "            print(\"|{} - Processed {}\".format(time.asctime(time.localtime(time.time())), num_idx), flush=True)\n",
        "        cuis_di[pssn] = []\n",
        "        cuis_li_tmp = []\n",
        "        date_li_tmp = []\n",
        "        for datetime_str in sorted(v.keys()): # sort by time\n",
        "            datetime_object = datetime.strptime(datetime_str, '%Y-%m-%d') # make sure time str is correct\n",
        "            infos = v[datetime_str]\n",
        "            if len(infos) > 0:\n",
        "                # cuis_di[pssn].append((cuis, ext_cuis, strs))\n",
        "                cuis_li_tmp.append((infos, [], []))\n",
        "                date_li_tmp.append(datetime_str)\n",
        "            for cui_id in infos:\n",
        "                dictionary[cui_id] += 1\n",
        "        if len(cuis_li_tmp) > 0:\n",
        "            cuis_di[pssn] = cuis_li_tmp\n",
        "            date_di[pssn] = date_li_tmp\n",
        "    return cuis_di, date_di, dictionary\n",
        "\n",
        "patients_few = dict(islice(patients.items(), 0, 200))\n",
        "# cuis, date, dictionary = icd2cui(patients_few, logging_step=50000)\n",
        "cuis, date, dictionary = icd2cui(patients, logging_step=50000)\n",
        "\n",
        "dir_apth = '/content/drive/My Drive/'\n",
        "print(\"Number of cui in dictionary: {}\".format(len(dictionary)), flush=True)\n",
        "with open(dir_apth + '/dict.txt', 'w') as handle: #TODO\n",
        "    handle.write(\"[PAD]\"+\"\\n\")\n",
        "    for i in range(99):\n",
        "        handle.write(\"[unused{}]\".format(i)+\"\\n\")\n",
        "    handle.write(\"[UNK]\"+\"\\n\")\n",
        "    handle.write(\"[CLS]\"+\"\\n\")\n",
        "    handle.write(\"[SEP]\"+\"\\n\")\n",
        "    handle.write(\"[MASK]\"+\"\\n\")\n",
        "    for i in range(99,194):\n",
        "        handle.write(\"[unused{}]\".format(i)+\"\\n\")\n",
        "    for k,v in dictionary.items():\n",
        "        handle.write(\"{}\\n\".format(k))\n",
        "# save data\n",
        "print(\"Saving patient data...\", flush=True)\n",
        "f1 = open(dir_apth + '/value.pickle', 'wb')\n",
        "f3 = open(dir_apth + '/dates.pickle', 'wb')\n",
        "f2 = open(dir_apth + '/key.txt', 'w')\n",
        "for k,v in cuis.items():\n",
        "    pickle.dump(v, f1, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    pickle.dump(date[k], f3, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    f2.write(\"{}\\n\".format(k))\n",
        "f1.close()\n",
        "f3.close()\n",
        "f2.close()\n",
        "\n",
        "#print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPnR5qTKctso",
        "outputId": "c81285f2-e3ef-4923-d3cc-a1148f6c97c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of cui in dictionary: 971\n",
            "Saving patient data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Data"
      ],
      "metadata": {
        "id": "c0Tqm1W8e43s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load key.txt, value.pickle and dates.pickle files\n",
        "# Code reference: https://github.com/whaleloops/TransformEHR/blob/main/sample_load.py\n",
        "\n",
        "dir_path = \"/content/drive/My Drive/\"\n",
        "do_date = True\n",
        "\n",
        "\n",
        "f1 = open(dir_path+ '/value.pickle', 'rb')\n",
        "f3 = open(dir_path+ '/dates.pickle', 'rb')\n",
        "f2 = open(dir_path+ '/key.txt', 'r')\n",
        "keys = f2.readlines()\n",
        "\n",
        "patients = {}\n",
        "for key in keys:\n",
        "    patient_idd = key.strip()\n",
        "    each_visit = pickle.load(f1)\n",
        "    f1obj = []\n",
        "    for (cuis, ext_cuis, strs) in each_visit:\n",
        "        f1obj.append((cuis, [], []))\n",
        "    f3obj = pickle.load(f3)\n",
        "    assert len(f1obj) == len(f3obj)\n",
        "    if do_date:\n",
        "        if patient_idd in patients:\n",
        "            patients[patient_idd] += list(zip(f1obj, f3obj))\n",
        "        else:\n",
        "            patients[patient_idd] = list(zip(f1obj, f3obj))\n",
        "    else:\n",
        "        if patient_idd in patients:\n",
        "            patients[patient_idd] += f1obj\n",
        "        else:\n",
        "            patients[patient_idd] = f1obj\n",
        "\n",
        "#print(\"Number of patients in the sample dataset\")\n",
        "print(f'Number of patients in the sample dataset : {len(patients)}')\n",
        "print(f'Patients : {patients}')\n",
        "#print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFwQQnWPe7cD",
        "outputId": "a36b0724-8202-4ea4-d87b-eff7623d9ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patients in the sample dataset : 100\n",
            "Patients : {'10000084': [((['10-G3183', '10-F0280', '10-R441', '10-R296', '10-E785', '10-Z8546'], [], []), '2160-11-25'), ((['10-R4182', '10-G20', '10-F0280', '10-R609', '10-E785', '10-Z8546'], [], []), '2160-12-28')], '10000117': [((['10-R1310', '10-R0989', '10-K31819', '10-K219', '10-K449', '10-F419', '10-I341', '10-M810', '10-Z87891'], [], []), '2181-11-15'), ((['10-S72012A', '10-W010XXA', '10-Y93K1', '10-Y92480', '10-K219', '10-E7800', '10-I341', '10-G43909', '10-Z87891', '10-Z87442', '10-F419', '10-M810', '10-Z7901'], [], []), '2183-09-21')], '10000980': [((['10-D500', '10-I5023', '10-N184', '10-E118', '10-K2970', '10-Z23', '10-K259', '10-K5730', '10-I2510', '10-Z87891', '10-I252', '10-Z955', '10-I129', '10-Z794', '10-Z8673', '10-R0789', '10-Z86718', '10-R791', '10-T45515A', '10-I70218', '10-K222', '10-K219'], [], []), '2191-05-24'), ((['10-I5023', '10-N184', '10-D631', '10-E1121', '10-Z86718', '10-I129', '10-Z955', '10-I2510', '10-Z7901', '10-Z794', '10-I340', '10-I252', '10-Z8673', '10-Z87891', '10-Z91128', '10-E785'], [], []), '2191-07-19'), ((['10-I130', '10-I5033', '10-E872', '10-N184', '10-E1122', '10-N2581', '10-I2510', '10-E11319', '10-D6489', '10-E785', '10-Z955', '10-Z86718', '10-I252', '10-Z2239', '10-G4700', '10-M1A9XX0', '10-R0902', '10-E1151', '10-Z794', '10-E669', '10-Z6831'], [], []), '2193-08-17')], '10001401': [((['10-C675', '10-I10', '10-D259', '10-Z87891', '10-E785', '10-E890'], [], []), '2131-06-15'), ((['10-T814XXA', '10-K651', '10-N179', '10-I82412', '10-C679', '10-I10', '10-B966', '10-R7881', '10-Y838', '10-Y9289', '10-F17210', '10-Z436', '10-Z90710', '10-D72829', '10-Z96652'], [], []), '2131-07-02'), ((['10-I2699', '10-I82412', '10-N390', '10-I471', '10-I10', '10-I872', '10-R918', '10-B952', '10-E039', '10-E785', '10-E876', '10-E8342', '10-G4700', '10-K5900', '10-Z66', '10-N63', '10-D509', '10-D638', '10-Z7901', '10-Z8551', '10-Z906', '10-Z87891', '10-Z96652'], [], []), '2131-08-04'), ((['10-T814XXA', '10-A419', '10-K651', '10-N179', '10-N1330', '10-D62', '10-I2782', '10-N138', '10-C679', '10-I10', '10-E785', '10-E039', '10-K439', '10-K435', '10-E876', '10-Y838', '10-Y929', '10-Z96652', '10-Z86718', '10-N63', '10-Z7901', '10-Z87891'], [], []), '2131-10-05'), ((['10-N99820', '10-E43', '10-R310', '10-N131', '10-D62', '10-R8271', '10-E039', '10-E785', '10-N9989', '10-I10', '10-Z86718', '10-Z936', '10-Z7902', '10-Z86711', '10-Z87891', '10-Z6822', '10-Z8551', '10-Z96652', '10-Y848', '10-Y833', '10-Y929'], [], []), '2131-11-15'), ((['10-T8140XA', '10-A4181', '10-R6520', '10-N179', '10-N1330', '10-N12', '10-T8144XA', '10-Z936', '10-I10', '10-E785', '10-E039', '10-Z87891', '10-Z8551', '10-Z86718', '10-Y848', '10-Y92239'], [], []), '2133-07-13')], '10001667': [((['10-R471', '10-I5030', '10-E538', '10-Z66', '10-I4891', '10-Z7902', '10-I110', '10-E7849', '10-K5790', '10-Z87891', '10-R946', '10-I455', '10-R413'], [], []), '2173-08-24')], '10001843': [((['10-T82855A', '10-I5031', '10-I2510', '10-Y840', '10-Y929', '10-I252', '10-I110', '10-E785', '10-E119', '10-Z794', '10-I4891', '10-Z7901', '10-E669', '10-Z6831', '10-K219'], [], []), '2131-11-11')], '10001884': [((['10-J441', '10-F17210', '10-R079'], [], []), '2130-06-24'), ((['10-K921', '10-D62', '10-I4891', '10-J449', '10-I10', '10-F17210', '10-D649', '10-I2510', '10-F419', '10-E785', '10-I739', '10-M47892', '10-R0602', '10-G4700', '10-H409', '10-K449', '10-K2970', '10-Z7982', '10-Z7901', '10-Z96649'], [], []), '2130-08-23'), ((['10-J441', '10-R0902', '10-E876', '10-I10', '10-E780', '10-E785', '10-Z87891', '10-Z9981'], [], []), '2130-10-06'), ((['10-I4891', '10-J441', '10-Z9981', '10-Z7901', '10-I10', '10-E785', '10-F419', '10-I739', '10-I2510', '10-Z87891', '10-D509', '10-R079', '10-G4700', '10-H409'], [], []), '2130-10-12'), ((['10-J441', '10-Z9981', '10-I10', '10-I4891', '10-Z7901', '10-Z87891'], [], []), '2130-10-14'), ((['10-J441', '10-I4892', '10-Z9981', '10-I480', '10-J45998', '10-Z87891', '10-I2510', '10-Z96649', '10-I10', '10-E785', '10-D509', '10-I739', '10-F419', '10-K5900', '10-M1990', '10-Z825', '10-Z8249', '10-Z7901'], [], []), '2130-10-22'), ((['10-J441', '10-M7989', '10-I2510', '10-I499', '10-I739', '10-F419', '10-Z87891', '10-E876'], [], []), '2130-11-20'), ((['10-J441', '10-I4892', '10-I248', '10-J45909', '10-Z87891', '10-I4891', '10-F419', '10-G4700', '10-E780', '10-I2510', '10-E876', '10-R312', '10-I739', '10-Z7952', '10-Z9981'], [], []), '2130-11-30'), ((['10-J441', '10-Z9981', '10-I4891', '10-J45909', '10-Z7901', '10-I10', '10-I2510', '10-E785', '10-M1990', '10-F419', '10-I739', '10-G4700', '10-D649', '10-Z96649', '10-Z87891'], [], []), '2130-12-08'), ((['10-J441', '10-J45909', '10-H6991', '10-F419', '10-G4700', '10-I4891', '10-Z7901', '10-I10', '10-I2510', '10-D649', '10-E785', '10-Z87891', '10-Z006'], [], []), '2130-12-24'), ((['10-J441', '10-N179', '10-Z9981', '10-I4891', '10-D649', '10-I10', '10-E785', '10-G5622', '10-I2510', '10-M1990', '10-Z96649', '10-Z87891', '10-J45909', '10-F419', '10-G4700', '10-R040', '10-I739'], [], []), '2130-12-30'), ((['10-J441', '10-K7200', '10-R579', '10-J9602', '10-J9601', '10-I442', '10-I82621', '10-I4891', '10-D696', '10-I469', '10-I10', '10-E785', '10-Z7901', '10-M47892', '10-I7389', '10-I2510', '10-Z87891', '10-Z96641', '10-J0190', '10-F419', '10-G4700', '10-R609', '10-D509', '10-R509', '10-M1990', '10-Y92239', '10-Z7952', '10-Z825', '10-Z781', '10-Z515', '10-Z66', '10-Z9981', '10-J45909', '10-I447', '10-M7981', '10-T45515A'], [], []), '2131-01-20')], '10001919': [((['10-C169', '10-C786', '10-I82621', '10-K9171', '10-Y838', '10-Y92234', '10-N359', '10-E039', '10-K219', '10-Z7901', '10-Z8546'], [], []), '2124-04-21')], '10002013': [((['10-R0789', '10-E876', '10-I25119', '10-I25709', '10-Z951', '10-Z955', '10-E1122', '10-E1165', '10-I129', '10-N183', '10-E785', '10-J449', '10-K219', '10-E1140', '10-F329', '10-E669', '10-Z6837', '10-G8929', '10-M25519', '10-G4733', '10-Z794', '10-I252', '10-Z87891'], [], []), '2164-03-19'), ((['10-I25110', '10-E1110', '10-E1122', '10-E11319', '10-M869', '10-I5032', '10-I130', '10-T82855A', '10-E11621', '10-E1142', '10-E1165', '10-E1169', '10-L97529', '10-N183', '10-J449', '10-B961', '10-B951', '10-F329', '10-K219', '10-G4733', '10-Z87891', '10-Z951', '10-Z794', '10-Z9114', '10-G2581', '10-Z955', '10-Y840', '10-Y929'], [], []), '2165-11-26'), ((['10-A419', '10-M86672', '10-M86172', '10-N179', '10-L03116', '10-J9811', '10-I5032', '10-I130', '10-E1152', '10-I96', '10-E1169', '10-L97524', '10-E11621', '10-B9561', '10-J449', '10-E11319', '10-E1142', '10-E1165', '10-I2510', '10-I252', '10-G4733', '10-K219', '10-G2581', '10-N189', '10-E1122', '10-Z794', '10-Z951', '10-Z955', '10-Z87891'], [], []), '2166-04-19'), ((['10-L03031', '10-E1140', '10-Z794', '10-I10', '10-Z89412', '10-E11319', '10-I2510', '10-Z955', '10-K219', '10-J449'], [], []), '2167-07-05')], '10002131': [((['10-I82412', '10-I5033', '10-E873', '10-E46', '10-I482', '10-G309', '10-I82432', '10-F0280', '10-I10', '10-E876', '10-M810', '10-Z66', '10-Z515', '10-Z993', '10-H9192', '10-H409', '10-M25551', '10-K5900', '10-Z6827', '10-Z85828'], [], []), '2128-03-19')], '10002221': [((['10-M1712', '10-D6861', '10-J449', '10-G4733', '10-F329', '10-E119', '10-M4696', '10-E785', '10-K219', '10-E669', '10-Z7901', '10-Z87891', '10-Z6837', '10-Z86711'], [], []), '2203-06-16'), ((['10-L7632', '10-C50912', '10-D6861', '10-K760', '10-K219', '10-G4733', '10-E785', '10-I671', '10-J449', '10-E119', '10-Y838', '10-F329', '10-Z7901', '10-Z86711', '10-Z87891', '10-Z803', '10-Z8041', '10-Z9012'], [], []), '2204-06-30'), ((['10-M7061', '10-D6861', '10-D509', '10-E559', '10-Z87891', '10-Z86711', '10-Z7901', '10-C50912', '10-G4733', '10-E785', '10-E119', '10-K219', '10-K5790', '10-F329', '10-J449', '10-I83811'], [], []), '2204-07-06'), ((['10-M5116', '10-D6861', '10-N390', '10-D62', '10-I83811', '10-D509', '10-Z87891', '10-Z86711', '10-Z7901', '10-E559', '10-C50912', '10-E785', '10-G4733', '10-E119', '10-K219', '10-K5790', '10-F329', '10-J449', '10-I671', '10-M48061', '10-K5900', '10-E876'], [], []), '2204-07-25')], '10002266': [((['10-O4202', '10-O411230', '10-M358', '10-O621', '10-O631', '10-Z3A39', '10-Z370', '10-R760', '10-O99284', '10-E039', '10-I7300'], [], []), '2131-03-06')], '10002315': [((['10-F329', '10-R45851'], [], []), '2161-03-24')], '10002348': [((['10-C7931', '10-G935', '10-G936', '10-G911', '10-C3490', '10-F05', '10-I10', '10-F17210', '10-G510', '10-M21372', '10-E039', '10-M810', '10-Z781', '10-R001', '10-R739', '10-T380X5A', '10-Y92239', '10-D72829', '10-R3915'], [], []), '2112-12-10')], '10002428': [((['10-K922', '10-E43', '10-M8008XA', '10-F0390', '10-Z681', '10-R000', '10-I10', '10-F329', '10-E039', '10-Z66', '10-Z931', '10-M3500', '10-Z5309', '10-I340'], [], []), '2160-04-18'), ((['10-S0990XA', '10-S4992XA', '10-S79912A', '10-W1839XA', '10-Z9181', '10-Y92129', '10-M3500', '10-H53143', '10-H04123', '10-H3530', '10-Z961', '10-E039', '10-M4856XA', '10-I10', '10-M5030', '10-E049'], [], []), '2160-07-16')], '10002430': [((['10-K4030', '10-I480', '10-I272', '10-I509', '10-I2510', '10-I10', '10-K219', '10-N400', '10-E785', '10-J439', '10-I4510', '10-Z7982', '10-Z951', '10-Z87891'], [], []), '2125-06-25'), ((['10-I5033', '10-I281', '10-I272', '10-I480', '10-E871', '10-J449', '10-I2781', '10-I2510', '10-K219', '10-I10', '10-N400', '10-E785', '10-Z902', '10-Z7982', '10-Z9861', '10-Z87891'], [], []), '2125-09-30'), ((['10-I130', '10-I5023', '10-N179', '10-C799', '10-N183', '10-Z87891', '10-J439', '10-Z951', '10-Z955', '10-I071', '10-I2720', '10-I480', '10-Z7902', '10-Z8673', '10-C439', '10-I6523', '10-K219', '10-I4510', '10-N400', '10-E7849', '10-Z902', '10-R740', '10-I255'], [], []), '2129-05-02'), ((['10-I5023', '10-J189', '10-J9691', '10-C7951', '10-J90', '10-N179', '10-I255', '10-I480', '10-Z7902', '10-I2720', '10-I079', '10-I2510', '10-Z951', '10-N183', '10-D638', '10-D696', '10-C439', '10-Z87891'], [], []), '2129-06-24')], '10002443': [((['10-I309', '10-J9602', '10-I314', '10-I480', '10-E119', '10-Z66', '10-I10', '10-M069', '10-E785', '10-Z86718', '10-Z87891'], [], []), '2183-10-20')], '10002495': [((['10-I214', '10-R570', '10-I509', '10-R578', '10-A047', '10-N179', '10-S3730XA', '10-I2510', '10-E118', '10-X58XXXA', '10-Y92239', '10-I10', '10-E785', '10-T45515A', '10-Z86718', '10-Z7901', '10-R310', '10-Z7902', '10-I480', '10-Z23', '10-K2960', '10-B9681', '10-Z87891', '10-I252', '10-R410', '10-K219'], [], []), '2141-05-29')], '10002528': [((['10-F4489', '10-R29818', '10-I7300', '10-I951', '10-R000', '10-F410', '10-E869', '10-K219', '10-D509', '10-D519', '10-F329'], [], []), '2168-12-20'), ((['10-R29818', '10-F411', '10-F329', '10-F4310', '10-F5082', '10-Z6822', '10-D649'], [], []), '2170-03-18')], '10002545': [((['10-O480', '10-O98813', '10-Z3A41', '10-Z370', '10-B951', '10-O701'], [], []), '2158-09-10')], '10002755': [((['10-S12201D', '10-S12601D', '10-G20', '10-E039', '10-K219', '10-W06XXXD'], [], []), '2141-12-14')], '10002800': [((['10-O9A112', '10-C50412', '10-O99512', '10-Y836', '10-E890', '10-O99282', '10-O26852', '10-Z3A14', '10-Z170', '10-J45909', '10-Z85850', '10-Z87891'], [], []), '2164-02-25'), ((['10-O99613', '10-K029', '10-Z3A34', '10-O99513', '10-J45998', '10-O9989', '10-O99013', '10-O99283', '10-E890', '10-Y836', '10-M170', '10-M479', '10-Z853', '10-Z85850', '10-Z87891', '10-K219'], [], []), '2164-07-14'), ((['10-O4593', '10-O411230', '10-O7020', '10-J4520', '10-O752', '10-O99284', '10-E890', '10-O99344', '10-O76', '10-O99824', '10-O745', '10-Z3A37', '10-Z370', '10-O7589', '10-Z853', '10-Z85850', '10-L732', '10-N39498', '10-Z87891'], [], []), '2164-08-04'), ((['10-Z421', '10-Z4001', '10-T85898A', '10-Y834', '10-Y92238', '10-J4520', '10-E890', '10-F329', '10-R509', '10-Z9012', '10-Z853', '10-Z85850'], [], []), '2165-01-28')], '10002807': [((['10-K8590', '10-I2510', '10-I10', '10-E785', '10-K219', '10-K5792', '10-Z951'], [], []), '2152-03-31')], '10002869': [((['10-O26892', '10-R002', '10-R0602', '10-O09522', '10-Z3A23', '10-Z8619', '10-O99342', '10-F419'], [], []), '2136-04-06'), ((['10-O34211', '10-D649', '10-O99344', '10-F419', '10-O9902', '10-Z3A39', '10-Z370', '10-Z302'], [], []), '2136-07-27')], '10002930': [((['10-F10239', '10-F1110', '10-R45851', '10-Z87820', '10-B1920', '10-Z23', '10-Z590', '10-F1410', '10-R509', '10-F29', '10-Z21', '10-Z9114', '10-D72819'], [], []), '2198-04-22'), ((['10-F1994', '10-B20', '10-F1490', '10-F1099', '10-F209', '10-Z87820', '10-B1920', '10-Z87891'], [], []), '2198-05-04'), ((['10-F29', '10-S0990XA', '10-F1910', '10-N390', '10-F1010', '10-F39', '10-F17200', '10-Z87820', '10-Z590', '10-Z21', '10-W19XXXA', '10-Y929'], [], []), '2199-02-19'), ((['10-E162', '10-F10129', '10-Z21', '10-B1920', '10-R4182'], [], []), '2200-06-05'), ((['10-F10129', '10-R4182', '10-E162', '10-B20', '10-B1920'], [], []), '2201-02-13'), ((['10-F39', '10-R45851', '10-M542', '10-B20', '10-Z9114', '10-F17210', '10-R51', '10-W19XXXA', '10-Y929', '10-B1920', '10-S0990XA', '10-D696'], [], []), '2201-03-26')], '10002976': [((['10-E1065', '10-Z794', '10-D72829', '10-E860', '10-I2510', '10-Z955', '10-E1042', '10-E103592', '10-E785', '10-I252'], [], []), '2153-08-14')], '10003019': [((['10-I340', '10-I5032', '10-Z006', '10-D8689', '10-E785', '10-J45909', '10-K219', '10-N400', '10-Z8042', '10-G4733', '10-Z8572', '10-Z87891', '10-Z9621', '10-H9190'], [], []), '2181-03-04')], '10003299': [((['10-I639', '10-G8194', '10-E119', '10-I672', '10-Z8673', '10-I10', '10-E785', '10-I252', '10-Z85038', '10-E538', '10-Z9181', '10-F17210', '10-I6529', '10-I2510', '10-R32', '10-R159', '10-M8580', '10-Z7902'], [], []), '2181-10-23'), ((['10-R042', '10-E210', '10-R918', '10-Z8673', '10-E119', '10-E785', '10-F17210', '10-I252', '10-Z85038', '10-J9819'], [], []), '2183-02-28'), ((['10-E876', '10-C3490', '10-J029', '10-Z8673', '10-I10', '10-E119', '10-Z85038', '10-F17210', '10-E785'], [], []), '2183-06-19'), ((['10-K208', '10-E870', '10-C342', '10-E46', '10-Y842', '10-Y92009', '10-D519', '10-E876', '10-R197', '10-F17210', '10-Z85038', '10-E119', '10-Z8673', '10-E785', '10-Z6822', '10-R32', '10-R1310'], [], []), '2183-07-01'), ((['10-K208', '10-E43', '10-I82C11', '10-C342', '10-D6869', '10-E512', '10-I69354', '10-R627', '10-K2950', '10-B9681', '10-E860', '10-K222', '10-I10', '10-K224', '10-K449', '10-K2980', '10-F17210', '10-Z66', '10-Z6822', '10-Y842', '10-Y92009', '10-Z9221', '10-Z7901', '10-Z85038', '10-Z9049'], [], []), '2183-08-01')], '10003372': [((['10-D497', '10-K740', '10-I10', '10-K219', '10-K660', '10-Z87891'], [], []), '2185-07-08')], '10003385': [((['10-K629', '10-L910', '10-I10', '10-D573', '10-E669', '10-Z6831', '10-Z87891'], [], []), '2131-09-26')], '10003400': [((['10-T8131XA', '10-R6521', '10-J9601', '10-N179', '10-A419', '10-G9340', '10-C9000', '10-J910', '10-K5660', '10-C786', '10-C7889', '10-C211', '10-E46', '10-Q620', '10-T814XXA', '10-T8359XA', '10-N12', '10-B3749', '10-K311', '10-I5032', '10-D62', '10-Z66', '10-Z515', '10-E8809', '10-I482', '10-Z7901', '10-M179', '10-I129', '10-B9562', '10-N183', '10-Z433', '10-R310', '10-E669', '10-Z6838', '10-Y848', '10-Y92239', '10-B965', '10-D696'], [], []), '2137-09-02')], '10003412': [((['10-M4856XA', '10-K913', '10-T8489XA', '10-M5136', '10-Y831', '10-Y92239', '10-Y838', '10-Y92009'], [], []), '2173-01-20')], '10003502': [((['10-R001', '10-I5033', '10-F0391', '10-I2582', '10-E871', '10-Z9181', '10-I2510', '10-I252', '10-I10', '10-I350', '10-I4891', '10-Z7902', '10-E785', '10-Z66'], [], []), '2169-08-28')], '10003637': [((['10-I429', '10-I2510', '10-I10', '10-Z951', '10-Z955', '10-I252', '10-Z7901', '10-Z8673'], [], []), '2145-11-24'), ((['10-K611', '10-I2582', '10-I509', '10-I2510', '10-I252', '10-Z950', '10-Z951', '10-Z8673', '10-E785', '10-I255', '10-F17210'], [], []), '2146-01-26'), ((['10-N179', '10-I5022', '10-I959', '10-E8339', '10-I2510', '10-Z951', '10-Z950', '10-Z8673', '10-D649', '10-F17210', '10-I10', '10-E785', '10-I255'], [], []), '2146-02-19'), ((['10-K603', '10-I130', '10-I5022', '10-N179', '10-N183', '10-E785', '10-I2510', '10-I480', '10-Z7682', '10-I255', '10-R590', '10-I2722', '10-I252', '10-Z7901', '10-Z8673', '10-Z95810', '10-Z951', '10-Z955', '10-Z87891'], [], []), '2149-05-20')], '10003731': [((['10-E6601', '10-Z6843', '10-K449', '10-K219', '10-G4733', '10-K2270', '10-I480', '10-Z7901', '10-I10', '10-E785'], [], []), '2152-04-14')], '10003757': [((['10-S066X0A', '10-G92', '10-N390', '10-W050XXA', '10-Y92009', '10-I2510', '10-I10', '10-I739', '10-J439', '10-Z96641', '10-B9620', '10-F0390', '10-I350', '10-D649', '10-D469', '10-S0181XA', '10-M4854XD', '10-J8410', '10-Z66', '10-M8448XD', '10-S7002XA', '10-R402142', '10-R402362', '10-R402252'], [], []), '2142-09-15')], '10004113': [((['10-D1802', '10-I619', '10-G40909'], [], []), '2173-03-22')], '10004296': [((['10-O133', '10-O722', '10-O8612', '10-O639', '10-L03311', '10-L02211', '10-O324XX0', '10-O99334', '10-O9952', '10-R609', '10-J45909', '10-O860', '10-Z3A37', '10-Z370'], [], []), '2168-11-08')], '10004322': [((['10-J189', '10-F200', '10-N179', '10-E1143', '10-J449', '10-Z794', '10-Z87891', '10-I951', '10-K219', '10-I2510', '10-I10', '10-E785', '10-Z9181'], [], []), '2134-10-02'), ((['10-N179', '10-R339', '10-E860', '10-F200', '10-I10', '10-E119', '10-F88', '10-K219', '10-E7800', '10-J449', '10-J45909', '10-Z794'], [], []), '2135-01-03'), ((['10-T83511A', '10-G92', '10-I10', '10-B9620', '10-J449', '10-E119', '10-F200', '10-N390', '10-Y846', '10-Y929', '10-Z87891', '10-K210', '10-I2510', '10-E785', '10-D649'], [], []), '2135-02-12')], '10004457': [((['10-I6521', '10-I2510', '10-Z955', '10-Z8546', '10-Z8571', '10-E785', '10-Z951', '10-I081', '10-Z952', '10-G4733'], [], []), '2147-12-21'), ((['10-I25110', '10-Z951', '10-Z955', '10-I110', '10-I5022', '10-E785', '10-Z8673', '10-Z7902', '10-R21', '10-J45909', '10-I4892', '10-I6522', '10-Z952', '10-Z8571', '10-Z8546', '10-Z87891', '10-Z8249'], [], []), '2148-09-15')], '10004606': [((['10-G40409', '10-K8510', '10-G9340', '10-K8064', '10-E871', '10-I701', '10-I10', '10-I160', '10-K219', '10-I739', '10-E785', '10-F17210', '10-Z95820', '10-Z86718', '10-Z8673'], [], []), '2159-03-06'), ((['10-N390', '10-K2971', '10-N179', '10-F05', '10-I701', '10-G40409', '10-I10', '10-D500', '10-I739', '10-Z720', '10-K219', '10-B9620', '10-K5903', '10-T402X5A', '10-Y92230', '10-Z86718'], [], []), '2159-03-22'), ((['10-K31811', '10-E440', '10-R569', '10-D62', '10-I150', '10-Z681', '10-N390', '10-E785', '10-I739', '10-I69398', '10-R531', '10-R2681', '10-R51', '10-R001', '10-T448X5A', '10-Y92230', '10-R109', '10-F1011', '10-Z86718', '10-Z87891'], [], []), '2159-04-11'), ((['10-K31811', '10-B1910', '10-S0990XA', '10-G629', '10-D62', '10-F1120', '10-I452', '10-I6523', '10-G40909', '10-I951', '10-F319', '10-Q2733', '10-I10', '10-W01198A', '10-Y92008', '10-I701', '10-M5416', '10-E039', '10-E785', '10-J449', '10-K219', '10-Z86718', '10-Z87891', '10-K2270', '10-R110', '10-T402X5A', '10-Y929', '10-I739', '10-I69398', '10-R531', '10-R42', '10-N3090', '10-R079', '10-I459', '10-K5900'], [], []), '2159-09-22')], '10004719': [((['10-T82868A', '10-I82441', '10-Y832', '10-Y92009', '10-I70411', '10-Z86718', '10-J45909'], [], []), '2183-09-03')], '10004720': [((['10-J690', '10-G931', '10-I82621', '10-Z8674', '10-J95811', '10-I959', '10-J9600', '10-I10', '10-D638', '10-Y848', '10-Y92238', '10-Z66', '10-Z515', '10-Z781', '10-F209', '10-S42291D', '10-W1830XD', '10-Z9181', '10-Z85828', '10-H269', '10-Z720', '10-L409', '10-M810', '10-L570', '10-M720', '10-J309', '10-F1021', '10-Z7902', '10-H6120'], [], []), '2186-11-17')], '10004764': [((['10-I2510', '10-I959', '10-I4891', '10-D649', '10-I10', '10-E7800', '10-Z955', '10-Z87891', '10-R0902'], [], []), '2168-04-16')], '10005001': [((['10-D259', '10-E8809', '10-N3289', '10-N8320', '10-N736', '10-Z90721', '10-Z9079', '10-N393', '10-Z975', '10-J309', '10-F329', '10-Z800', '10-Z8042'], [], []), '2164-10-14'), ((['10-C562', '10-C786', '10-C772', '10-C787', '10-E669', '10-F419', '10-F329', '10-Z6836'], [], []), '2165-06-18')], '10005123': [((['10-K7460', '10-I81', '10-N179', '10-K766', '10-I8510', '10-K740', '10-D6959', '10-E785', '10-I864'], [], []), '2129-07-05'), ((['10-J95821', '10-I81', '10-T82868A', '10-K766', '10-I8510', '10-K740', '10-I864', '10-D6959', '10-E785', '10-Y838', '10-K7460', '10-Y92238'], [], []), '2129-08-19')], '10005236': [((['10-S72001A', '10-E119', '10-J449', '10-I10', '10-W1839XA', '10-Y92009', '10-Z87891', '10-I70209', '10-Z23'], [], []), '2180-07-03')], '10005308': [((['10-S82851A', '10-W108XXA', '10-Y92009'], [], []), '2178-04-20')], '10005606': [((['10-S12390A', '10-J9690', '10-S27321A', '10-S14109A', '10-S2220XA', '10-F10121', '10-A0472', '10-G960', '10-S22020A', '10-S12490A', '10-S12590A', '10-S12690A', '10-S93409A', '10-W109XXA', '10-Y9289', '10-S0101XA', '10-F17210', '10-F1290', '10-D649', '10-M5382'], [], []), '2143-12-16'), ((['10-K920', '10-N179', '10-F1010', '10-I10', '10-F17200', '10-E8352', '10-E876', '10-E8342', '10-K7030', '10-D6959'], [], []), '2144-03-08')], '10005749': [((['10-E1165', '10-N179', '10-E1122', '10-I130', '10-N189', '10-I5022', '10-N390', '10-E871', '10-E876', '10-I482', '10-F419', '10-Z940', '10-Z794', '10-Z7901', '10-E785'], [], []), '2145-09-14')], '10005817': [((['10-J9621', '10-J910', '10-J189', '10-N170', '10-E883', '10-I82412', '10-C7B8', '10-I5022', '10-I871', '10-C7A8', '10-J449', '10-I2510', '10-F17210', '10-Z951', '10-Z955', '10-E119', '10-Z85048', '10-F4310', '10-I4891', '10-Z515', '10-I129', '10-N189', '10-R410', '10-Z9981', '10-Z7902'], [], []), '2135-01-19')], '10005858': [((['10-N62', '10-N6082', '10-N6081', '10-E6601', '10-Z6842', '10-E039', '10-E042', '10-G8929', '10-D869', '10-I10', '10-G4733', '10-M4802', '10-K589', '10-M170', '10-M545', '10-F329', '10-M25531', '10-R7303', '10-Z96653', '10-Z8679', '10-Z833', '10-Z8249', '10-Z803'], [], []), '2177-02-11'), ((['10-I5031', '10-Z6843', '10-R079', '10-D649', '10-E669', '10-E039', '10-R413', '10-K219', '10-G2581', '10-G4700', '10-H04129', '10-R3915'], [], []), '2177-07-21')], '10005866': [((['10-K565', '10-E43', '10-J189', '10-J952', '10-K766', '10-M96831', '10-D62', '10-T814XXA', '10-L03311', '10-K7031', '10-M47817', '10-Z6823', '10-F17210', '10-D696', '10-Y838', '10-Y92239', '10-T17990A', '10-R339'], [], []), '2148-03-21'), ((['10-A084', '10-I81', '10-R188', '10-E872', '10-K766', '10-E871', '10-K7469', '10-K7290', '10-Z720'], [], []), '2149-02-14'), ((['10-K565', '10-K7031', '10-I8510', '10-K766', '10-F17210', '10-Z6823', '10-B1920', '10-I81', '10-E43'], [], []), '2149-06-25'), ((['10-K56600', '10-K766', '10-I8510', '10-K7031', '10-F17210', '10-M47818'], [], []), '2149-09-19'), ((['10-K660', '10-E43', '10-K766', '10-Z720', '10-K7031', '10-Z6820', '10-R1084'], [], []), '2149-09-26'), ((['10-K265', '10-E43', '10-J811', '10-D688', '10-K766', '10-D6959', '10-D62', '10-I471', '10-I959', '10-K255', '10-I4891', '10-K209', '10-B182', '10-K7031', '10-F1011', '10-K660', '10-K3189', '10-Z5309', '10-G8929', '10-I8510', '10-Z66', '10-Z515', '10-R0902', '10-D638', '10-Z590', '10-Z6824', '10-K449', '10-Z720'], [], []), '2149-10-25')], '10005909': [((['10-I82422', '10-I871', '10-I9751', '10-Y838', '10-Y92234'], [], []), '2144-11-02')], '10006029': [((['10-K831', '10-I81', '10-C7800', '10-K862', '10-C771', '10-E785', '10-I129', '10-E1122', '10-N189', '10-F419', '10-N400', '10-D539', '10-F329', '10-E039', '10-Z85528', '10-Z905', '10-Z87891', '10-Z800', '10-Z807', '10-Z8041'], [], []), '2169-08-22'), ((['10-A408', '10-K831', '10-I81', '10-J189', '10-K830', '10-C7802', '10-C7801', '10-C771', '10-N179', '10-C249', '10-R6520', '10-F419', '10-N400', '10-Z85528', '10-E785', '10-Z87891', '10-D539', '10-F329', '10-E039', '10-E1122', '10-I129', '10-N189', '10-Z794'], [], []), '2169-10-05'), ((['10-N179', '10-C221', '10-C259', '10-C7802', '10-C7801', '10-I10', '10-E785', '10-F419', '10-N400', '10-I129', '10-E1122', '10-N189', '10-Z7984', '10-Z87891', '10-T464X5A', '10-Y929', '10-D6481', '10-T451X5A', '10-E039', '10-Z794', '10-Z905', '10-I959', '10-E860', '10-Z7901'], [], []), '2170-07-12'), ((['10-C786', '10-E43', '10-C250', '10-C787', '10-C781', '10-C771', '10-N179', '10-E871', '10-G893', '10-K8681', '10-Z85528', '10-Z8589', '10-Z905', '10-Z66', '10-Z515', '10-E1122', '10-I129', '10-N189', '10-Z794', '10-E7849', '10-N400', '10-K529', '10-Z87891', '10-K117', '10-D630', '10-E860', '10-Z6826', '10-Z7902', '10-B001', '10-F419', '10-G4700', '10-R739', '10-T380X5A', '10-Y92230', '10-K5900'], [], []), '2170-08-11')], '10006269': [((['10-B003', '10-C20', '10-K626', '10-K2960', '10-K2980', '10-I10', '10-F329', '10-D508'], [], []), '2124-07-05')], '10006431': [((['10-C250', '10-H905', '10-I10', '10-E042', '10-K838', '10-L298', '10-K219', '10-D509', '10-G4700', '10-Z800'], [], []), '2128-01-16'), ((['10-K521', '10-K831', '10-C250', '10-D6959', '10-E860', '10-I10', '10-K219', '10-R110', '10-T451X5A', '10-Y929'], [], []), '2128-03-07'), ((['10-E860', '10-C250', '10-K521', '10-D701', '10-H905', '10-I10', '10-K219', '10-R112', '10-T451X5A', '10-Z6823', '10-R630', '10-E7800', '10-F419', '10-Y929'], [], []), '2128-03-30'), ((['10-K819', '10-K821', '10-C250', '10-E785', '10-I10', '10-K219', '10-K8681'], [], []), '2128-06-12'), ((['10-G893', '10-Z66', '10-K8580', '10-E43', '10-C250', '10-C787', '10-C7900', '10-I959', '10-K219', '10-Z681', '10-I10', '10-E049', '10-E7800', '10-K5900'], [], []), '2129-01-30'), ((['10-C250', '10-C787', '10-E860', '10-H903', '10-K219', '10-I10', '10-D473', '10-D72829', '10-E861', '10-R112', '10-Z66', '10-G893', '10-Z800', '10-R634', '10-Z6821'], [], []), '2129-02-06')], '10006457': [((['10-N179', '10-I69351', '10-E1121', '10-E1140', '10-E11649', '10-E113393', '10-I10', '10-E785', '10-D649', '10-F17210', '10-Z7984', '10-N141', '10-T39395A', '10-Y929'], [], []), '2151-11-08'), ((['10-R51', '10-R112', '10-N390', '10-E860', '10-R944', '10-E785', '10-I10', '10-E11319', '10-E1142', '10-R809', '10-E538', '10-Z87891', '10-I69351', '10-I69398'], [], []), '2152-12-29'), ((['10-E1165', '10-R109', '10-R112', '10-R42', '10-E1122', '10-I129', '10-N189', '10-Z8673', '10-E872'], [], []), '2154-01-29')], '10006513': [((['10-N136', '10-B9620', '10-N179', '10-E119', '10-Z794'], [], []), '2125-05-07'), ((['10-M10072', '10-N179', '10-M10062', '10-E1122', '10-N189', '10-E860', '10-Z87442', '10-Z794'], [], []), '2127-03-28')], '10006630': [((['10-F329', '10-F419', '10-R45851', '10-F909'], [], []), '2177-03-01')], '10006640': [((['10-D134', '10-G8918', '10-R112', '10-R0789', '10-Z800', '10-E119', '10-Z794', '10-R748', '10-M797', '10-K589', '10-F419', '10-E669', '10-K219', '10-G4700'], [], []), '2173-12-19')], '10006825': [((['10-F29', '10-F319'], [], []), '2157-05-28')], '10007058': [((['10-I214', '10-I7102', '10-K219', '10-Z23', '10-Z7902', '10-Z7982'], [], []), '2167-11-11')], '10007134': [((['10-S270XXA', '10-S2242XA', '10-Y33XXXA', '10-Y92481', '10-Z590', '10-F1010', '10-F1210', '10-Z720'], [], []), '2140-05-24')], '10007232': [((['10-O701', '10-Z370', '10-Z3A39'], [], []), '2120-06-07')], '10007266': [((['10-F332', '10-R45851', '10-E7800', '10-F411', '10-Z9149'], [], []), '2189-01-20')], '10007818': [((['10-K7469', '10-K767', '10-J9600', '10-A4151', '10-N179', '10-K661', '10-R571', '10-R6521', '10-K659', '10-N183', '10-E43', '10-K7201', '10-T8119XA', '10-K7200', '10-D684', '10-J95851', '10-R188', '10-D62', '10-I4892', '10-K766', '10-E871', '10-N390', '10-K567', '10-E872', '10-J9811', '10-I97610', '10-T80219A', '10-K7581', '10-I129', '10-D638', '10-I25118', '10-Z955', '10-E119', '10-Z794', '10-R627', '10-I4891', '10-B9689', '10-B9561', '10-Y848'], [], []), '2146-07-12')], '10007920': [((['10-F10129', '10-F22', '10-F329', '10-I10', '10-N486', '10-F1590', '10-K7030', '10-B20', '10-Z590'], [], []), '2141-08-08')], '10008077': [((['10-I714', '10-I482', '10-I10', '10-I2510', '10-M549', '10-Z7901', '10-Z006', '10-Z951', '10-K219', '10-M1990', '10-Z96651', '10-Z87891', '10-E669', '10-Z6837'], [], []), '2189-03-01'), ((['10-I25118', '10-I9751', '10-I25718', '10-N183', '10-I129', '10-I2582', '10-I480', '10-E785', '10-K219', '10-Y658', '10-Y713', '10-Y92238', '10-Z87891', '10-Z951', '10-Z7902'], [], []), '2191-03-07')], '10008245': [((['10-S32391A', '10-E870', '10-F72', '10-G40909', '10-R509', '10-J45909', '10-M810', '10-R2681', '10-X58XXXA', '10-Y92098', '10-K5900'], [], []), '2174-04-21')], '10008287': [((['10-D1802', '10-G40909', '10-Z85850'], [], []), '2145-10-02')], '10008647': [((['10-S065X9A', '10-S0081XA', '10-W19XXXA', '10-Y92009', '10-R55', '10-I10', '10-M542', '10-R531'], [], []), '2118-04-01')], '10008742': [((['10-I472', '10-F319', '10-R0600', '10-F4324', '10-F17210', '10-F1010'], [], []), '2173-06-26')], '10008816': [((['10-T43621A', '10-T43591A', '10-T528X1A', '10-Y929', '10-R972', '10-F17210'], [], []), '2128-12-19'), ((['10-F10129', '10-I10', '10-R569', '10-K859', '10-R011', '10-F4310', '10-R4182'], [], []), '2129-04-04'), ((['10-F10129', '10-R4182', '10-I10', '10-R569', '10-R011', '10-F4310', '10-F319', '10-F339'], [], []), '2129-04-05'), ((['10-R45851', '10-F10129', '10-F339', '10-M542', '10-G40909', '10-F1421', '10-F1221'], [], []), '2129-05-17'), ((['10-R45851', '10-T50902A', '10-F10129', '10-F339', '10-F419', '10-F319', '10-Y929'], [], []), '2129-05-21'), ((['10-T426X1A', '10-F10129', '10-F329', '10-F17210', '10-Y929'], [], []), '2129-07-09'), ((['10-F10229', '10-S0990XA', '10-F329', '10-F14129', '10-F11129', '10-F1320', '10-F1520', '10-W109XXA', '10-Y929'], [], []), '2129-08-23'), ((['10-F10239', '10-F10229', '10-R072'], [], []), '2130-08-21'), ((['10-F10129', '10-F1910'], [], []), '2130-11-28'), ((['10-F10129', '10-R569'], [], []), '2132-03-15')], '10008819': [((['10-K436', '10-I10', '10-F17210'], [], []), '2169-08-25')], '10009129': [((['10-S68522A', '10-B182', '10-W312XXA', '10-Y9269', '10-F1290'], [], []), '2183-10-23')], '10010038': [((['10-J029', '10-J36', '10-R509'], [], []), '2167-03-18')], '10010058': [((['10-N179', '10-G9341', '10-E43', '10-E872', '10-D696', '10-I130', '10-I4891', '10-I5022', '10-E871', '10-Z681', '10-E860', '10-E785', '10-I2510', '10-I252', '10-I255', '10-N189', '10-M109', '10-R627', '10-Z7902', '10-D638'], [], []), '2145-10-03'), ((['10-I214', '10-I5023', '10-I7101', '10-I130', '10-N179', '10-I252', '10-I2510', '10-I5084', '10-N189', '10-R627', '10-E875', '10-I482', '10-Z7901', '10-L270', '10-I255', '10-G629', '10-D649', '10-R7989', '10-Z66', '10-Z95810', '10-T465X5A', '10-Y92230'], [], []), '2147-01-06'), ((['10-I2109', '10-J9601', '10-I5023', '10-I442', '10-R570', '10-I2510', '10-I255', '10-I482', '10-Z7901', '10-N189', '10-E785', '10-M109', '10-N400', '10-M1990', '10-Z66', '10-E875', '10-D649', '10-R34', '10-I252', '10-I081', '10-Z4502', '10-I2720', '10-Z781'], [], []), '2147-11-19')], '10010150': [((['10-O701', '10-Z370', '10-D573', '10-Z23', '10-O9902', '10-Z3A38'], [], []), '2121-04-05')], '10010231': [((['10-C9200', '10-E854', '10-D701', '10-K1230', '10-L99', '10-K219', '10-R0781', '10-R42', '10-R300', '10-R197', '10-T451X5A', '10-R5081', '10-Y92239'], [], []), '2117-12-05'), ((['10-Z5111', '10-C9200'], [], []), '2117-12-23'), ((['10-D709', '10-C9200', '10-R5081', '10-R1011', '10-M79662', '10-M79661', '10-L729'], [], []), '2118-01-08'), ((['10-Z5111', '10-C9200', '10-Z91048'], [], []), '2118-01-20'), ((['10-D701', '10-C9200', '10-T451X5A', '10-R5081', '10-Y92009', '10-D61818'], [], []), '2118-02-07'), ((['10-Z5111', '10-D61810', '10-C92Z0', '10-R740', '10-K5903', '10-T451X5A', '10-Y92230'], [], []), '2118-02-26'), ((['10-K2960', '10-D61810', '10-C92Z0', '10-K219', '10-E860', '10-T451X5A', '10-Y9289', '10-Z79899'], [], []), '2118-03-09'), ((['10-Z5111', '10-C92Z0', '10-K760', '10-K219', '10-K2970'], [], []), '2118-04-07'), ((['10-A4101', '10-D61810', '10-L03221', '10-E871', '10-C92Z1', '10-L731', '10-R5081', '10-T451X5A', '10-Y92009', '10-R918', '10-R740', '10-T50995A', '10-Y92230'], [], []), '2118-05-09')], '10010393': [((['10-R338', '10-A084', '10-T398X5A', '10-G588', '10-E039', '10-F450', '10-M545', '10-I951', '10-Z9689', '10-Y929'], [], []), '2136-07-02'), ((['10-G588', '10-I10', '10-R338', '10-R202', '10-Z9689', '10-G43909', '10-E039', '10-F459', '10-H6691', '10-I951', '10-K219', '10-K5900'], [], []), '2136-07-07'), ((['10-G588', '10-E039', '10-F419', '10-K828', '10-I498', '10-K219', '10-Z9689', '10-F450'], [], []), '2136-11-13')], '10010471': [((['10-I4891', '10-N186', '10-J189', '10-Z7682', '10-I248', '10-J449', '10-Z992', '10-I2510', '10-I350', '10-I340', '10-D631', '10-E039', '10-M5489', '10-M25512', '10-Z79891', '10-G8929'], [], []), '2155-05-10'), ((['10-I214', '10-N186', '10-I462', '10-J9690', '10-I5033', '10-S2243XA', '10-I959', '10-R7881', '10-I132', '10-I480', '10-Z9981', '10-J449', '10-I080', '10-I447', '10-E785', '10-I2510', '10-E039', '10-M5489', '10-Z87891', '10-K219', '10-Z45018', '10-D631', '10-Z992', '10-Y848', '10-Y9289', '10-R740', '10-Z515', '10-M7981', '10-T45515A', '10-Y92239'], [], []), '2155-12-07')], '10010655': [((['10-F329', '10-R45851', '10-F4310', '10-Z915', '10-Z62810'], [], []), '2167-03-08'), ((['10-F338', '10-F4310', '10-F419'], [], []), '2167-03-21'), ((['10-F339', '10-F4310', '10-Z915', '10-R45851'], [], []), '2167-05-11'), ((['10-F4310', '10-R451', '10-F329', '10-Z915', '10-S50812A', '10-S50811A', '10-X789XXA', '10-Y929'], [], []), '2167-07-14'), ((['10-F329', '10-S61512A', '10-S61511A', '10-X788XXA', '10-Y929', '10-F4310', '10-F449', '10-F603'], [], []), '2167-07-27'), ((['10-F329', '10-R45851', '10-F4310', '10-N390', '10-J111', '10-F419', '10-F603'], [], []), '2167-08-29'), ((['10-J111', '10-R45851', '10-F4310', '10-F603', '10-L309', '10-F329', '10-F39'], [], []), '2167-09-01'), ((['10-F329', '10-R45851', '10-F4310', '10-F603'], [], []), '2168-03-29')], '10010663': [((['10-J869', '10-J154', '10-J90', '10-E872', '10-J9811', '10-D649', '10-B955', '10-E860', '10-E8339', '10-E8342', '10-E876', '10-E8809', '10-K219'], [], []), '2146-10-13')], '10010718': [((['10-I481', '10-I5023', '10-N179', '10-I130', '10-I2510', '10-E785', '10-M06042', '10-M06041', '10-M109', '10-M810', '10-E559', '10-D649', '10-I255', '10-M19012', '10-M19011', '10-Z955', '10-Z7902', '10-Z87891'], [], []), '2169-01-27')], '10010848': [((['10-R51', '10-A812', '10-B20', '10-I10', '10-E785', '10-Z87891', '10-Z8673'], [], []), '2169-01-28')], '10010867': [((['10-S12600A', '10-S271XXA', '10-S066X0A', '10-J9600', '10-S240XXA', '10-F05', '10-S22049A', '10-S22059A', '10-S22069A', '10-S32029A', '10-Z6841', '10-D62', '10-J95851', '10-R339', '10-F10129', '10-F1290', '10-E669', '10-F329', '10-Z781', '10-V270XXA', '10-Y929', '10-S8012XA', '10-Z23', '10-Y848', '10-Y92239', '10-F419'], [], []), '2148-01-11'), ((['10-S22059A', '10-F329', '10-S22069A', '10-V499XXA', '10-Y929', '10-E669', '10-Z6833', '10-Z981', '10-J45909', '10-G43909', '10-F1910', '10-M40209'], [], []), '2148-01-30'), ((['10-J90', '10-I272', '10-M25511', '10-Z981', '10-Z87820', '10-V8609XS', '10-Y92488', '10-J45909', '10-E041', '10-G43909', '10-F1210'], [], []), '2148-03-13')], '10010993': [((['10-S02652A', '10-S2242XA', '10-Y09', '10-Y929', '10-Z23', '10-Z87891'], [], []), '2116-05-15')], '10010997': [((['10-T814XXA', '10-L0889', '10-B952', '10-B9561', '10-Y838', '10-Y92018', '10-L608', '10-M069', '10-I10', '10-K219', '10-L820', '10-M6289'], [], []), '2139-05-02')], '10011189': [((['10-R55', '10-H539', '10-H9319', '10-I10', '10-K625', '10-I720', '10-L409', '10-K219', '10-I951'], [], []), '2188-02-26'), ((['10-I471', '10-I671', '10-I10', '10-I2510', '10-K219', '10-I951', '10-D649', '10-Z598', '10-E785'], [], []), '2188-03-25'), ((['10-I471', '10-E785', '10-I959', '10-I10', '10-Z8711', '10-I2510', '10-Z8673', '10-L409'], [], []), '2189-07-04')], '10011259': [((['10-F329', '10-F419', '10-R45851', '10-G4700'], [], []), '2161-07-03'), ((['10-F339', '10-R45851', '10-I10', '10-F42', '10-F419', '10-F459', '10-Z9049', '10-J45909', '10-G4700', '10-E039', '10-E663', '10-Z6837'], [], []), '2161-07-08')], '10011279': [((['10-K047', '10-F17210'], [], []), '2179-05-03')], '10011365': [((['10-F322', '10-I69351', '10-R45851', '10-N390', '10-Z681', '10-I10', '10-N3941', '10-I69391', '10-R1310', '10-E7800', '10-I252', '10-I2510', '10-I69322', '10-R634'], [], []), '2166-01-28'), ((['10-A419', '10-J9601', '10-J690', '10-R6521', '10-J189', '10-R64', '10-R1312', '10-I69351', '10-I4891', '10-N390', '10-Z681', '10-Z66', '10-Z515', '10-Z781', '10-T18128A', '10-X58XXXA', '10-Y929', '10-I69391', '10-I10', '10-I2510', '10-E785', '10-F329', '10-I447', '10-I69322', '10-I7300', '10-E7800', '10-K5900', '10-I69398', '10-R2689', '10-Z87891', '10-B9620', '10-N3941'], [], []), '2166-02-20')], '10011427': [((['10-K7040', '10-N179', '10-K766', '10-N390', '10-K9181', '10-N281', '10-K7031', '10-I129', '10-N189', '10-B9620', '10-E869', '10-E785', '10-K219', '10-Y848', '10-Y92530'], [], []), '2135-12-26'), ((['10-K767', '10-J690', '10-E43', '10-A419', '10-R6521', '10-E872', '10-N390', '10-E871', '10-N179', '10-K7040', '10-N189', '10-I129', '10-K7031', '10-K219', '10-B9562', '10-M109', '10-D6959', '10-D631', '10-D539', '10-E785', '10-F1021', '10-Z6823'], [], []), '2136-02-12'), ((['10-K7040', '10-N186', '10-K767', '10-I120', '10-E440', '10-K7031', '10-M109', '10-K219', '10-D6959', '10-D631', '10-F1010', '10-Z992', '10-Z6825'], [], []), '2136-03-17'), ((['10-T80211A', '10-A4181', '10-R6521', '10-N186', '10-K767', '10-G9341', '10-T8242XA', '10-Z1622', '10-D684', '10-E871', '10-I120', '10-E440', '10-D61818', '10-Y718', '10-Y929', '10-Z992', '10-K7031', '10-D638', '10-D696', '10-E559', '10-E785', '10-Z6823', '10-K219', '10-M109', '10-K668', '10-I9589', '10-Z781', '10-E876', '10-K7040'], [], []), '2136-04-08')], '10011449': [((['10-L03115', '10-B955', '10-E039', '10-L309'], [], []), '2135-12-01')], '10011607': [((['10-R002', '10-B379', '10-R51', '10-R079', '10-I10', '10-F329'], [], []), '2185-01-27'), ((['10-S12300A', '10-S0990XA', '10-S0512XA', '10-W1809XA', '10-Y929', '10-J45909', '10-I10', '10-E785', '10-R7302', '10-M810', '10-F0390'], [], []), '2185-03-19')], '10011668': [((['10-I214', '10-I110', '10-I5023', '10-I428', '10-R0789', '10-E785', '10-R0902', '10-F329', '10-R300', '10-R9431', '10-E806', '10-R791', '10-Z7901', '10-Z952', '10-Z87891'], [], []), '2141-04-20')], '10011912': [((['10-S82142A', '10-B1920', '10-C50919', '10-W03XXXA', '10-Y92520', '10-J45909', '10-K7030', '10-E785', '10-F329', '10-M810', '10-F17200', '10-F1021'], [], []), '2176-10-21'), ((['10-M4854XA', '10-R030', '10-F329', '10-M810', '10-E785', '10-B1920', '10-K7460', '10-J45909'], [], []), '2177-10-05'), ((['10-R627', '10-E43', '10-Z681', '10-A048', '10-K7030', '10-B1920', '10-F1011', '10-Z853', '10-K2970', '10-F1911', '10-F1290', '10-J45909', '10-E785', '10-M810', '10-I10', '10-Z880', '10-Z882', '10-Z888', '10-K529', '10-G4700', '10-R000', '10-H53141', '10-R51', '10-L539', '10-M549', '10-Z87311', '10-K5900'], [], []), '2179-09-25')], '10011938': [((['10-S80212A', '10-R609', '10-R2681', '10-J449', '10-I509', '10-Q282', '10-G40909', '10-Z9981', '10-W19XXXA', '10-Y929'], [], []), '2128-12-23'), ((['10-G40802', '10-L89623', '10-L02214', '10-N390', '10-L03314', '10-B9620', '10-R310', '10-I10', '10-J449', '10-D509', '10-Z87891', '10-L98419'], [], []), '2132-01-30')], '10012206': [((['10-K8510', '10-E43', '10-N179', '10-D684', '10-I82890', '10-K8010', '10-E861', '10-K269', '10-I10', '10-G4733', '10-E1140', '10-Z7984', '10-F17210', '10-G250', '10-E8342', '10-D6959', '10-E781', '10-Z6822', '10-B957'], [], []), '2127-07-14')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train Test Data Preparation"
      ],
      "metadata": {
        "id": "E3cIzBCTtkQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the Subset dataset into Train and Test (80/20 split)\n",
        "s = pd.Series(patients )\n",
        "train_dataset , test_dataset  = [i.to_dict() for i in train_test_split(s, train_size=0.8)]\n",
        "print(\"len train:\", len(json.dumps(train_dataset, indent=4)))\n",
        "print(f'Train dataset: {train_dataset}')\n",
        "print(\"len test:\", len(test_dataset))\n",
        "print(f'Test dataset: {test_dataset}')"
      ],
      "metadata": {
        "id": "HO2GLLSstPf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38328dca-76e4-4ecf-d840-be9928158592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len train: 80\n",
            "Train dataset: {'10000980': [((['10-D500', '10-I5023', '10-N184', '10-E118', '10-K2970', '10-Z23', '10-K259', '10-K5730', '10-I2510', '10-Z87891', '10-I252', '10-Z955', '10-I129', '10-Z794', '10-Z8673', '10-R0789', '10-Z86718', '10-R791', '10-T45515A', '10-I70218', '10-K222', '10-K219'], [], []), '2191-05-24'), ((['10-I5023', '10-N184', '10-D631', '10-E1121', '10-Z86718', '10-I129', '10-Z955', '10-I2510', '10-Z7901', '10-Z794', '10-I340', '10-I252', '10-Z8673', '10-Z87891', '10-Z91128', '10-E785'], [], []), '2191-07-19'), ((['10-I130', '10-I5033', '10-E872', '10-N184', '10-E1122', '10-N2581', '10-I2510', '10-E11319', '10-D6489', '10-E785', '10-Z955', '10-Z86718', '10-I252', '10-Z2239', '10-G4700', '10-M1A9XX0', '10-R0902', '10-E1151', '10-Z794', '10-E669', '10-Z6831'], [], []), '2193-08-17')], '10005817': [((['10-J9621', '10-J910', '10-J189', '10-N170', '10-E883', '10-I82412', '10-C7B8', '10-I5022', '10-I871', '10-C7A8', '10-J449', '10-I2510', '10-F17210', '10-Z951', '10-Z955', '10-E119', '10-Z85048', '10-F4310', '10-I4891', '10-Z515', '10-I129', '10-N189', '10-R410', '10-Z9981', '10-Z7902'], [], []), '2135-01-19')], '10002755': [((['10-S12201D', '10-S12601D', '10-G20', '10-E039', '10-K219', '10-W06XXXD'], [], []), '2141-12-14')], '10005308': [((['10-S82851A', '10-W108XXA', '10-Y92009'], [], []), '2178-04-20')], '10002800': [((['10-O9A112', '10-C50412', '10-O99512', '10-Y836', '10-E890', '10-O99282', '10-O26852', '10-Z3A14', '10-Z170', '10-J45909', '10-Z85850', '10-Z87891'], [], []), '2164-02-25'), ((['10-O99613', '10-K029', '10-Z3A34', '10-O99513', '10-J45998', '10-O9989', '10-O99013', '10-O99283', '10-E890', '10-Y836', '10-M170', '10-M479', '10-Z853', '10-Z85850', '10-Z87891', '10-K219'], [], []), '2164-07-14'), ((['10-O4593', '10-O411230', '10-O7020', '10-J4520', '10-O752', '10-O99284', '10-E890', '10-O99344', '10-O76', '10-O99824', '10-O745', '10-Z3A37', '10-Z370', '10-O7589', '10-Z853', '10-Z85850', '10-L732', '10-N39498', '10-Z87891'], [], []), '2164-08-04'), ((['10-Z421', '10-Z4001', '10-T85898A', '10-Y834', '10-Y92238', '10-J4520', '10-E890', '10-F329', '10-R509', '10-Z9012', '10-Z853', '10-Z85850'], [], []), '2165-01-28')], '10007920': [((['10-F10129', '10-F22', '10-F329', '10-I10', '10-N486', '10-F1590', '10-K7030', '10-B20', '10-Z590'], [], []), '2141-08-08')], '10008287': [((['10-D1802', '10-G40909', '10-Z85850'], [], []), '2145-10-02')], '10007266': [((['10-F332', '10-R45851', '10-E7800', '10-F411', '10-Z9149'], [], []), '2189-01-20')], '10010058': [((['10-N179', '10-G9341', '10-E43', '10-E872', '10-D696', '10-I130', '10-I4891', '10-I5022', '10-E871', '10-Z681', '10-E860', '10-E785', '10-I2510', '10-I252', '10-I255', '10-N189', '10-M109', '10-R627', '10-Z7902', '10-D638'], [], []), '2145-10-03'), ((['10-I214', '10-I5023', '10-I7101', '10-I130', '10-N179', '10-I252', '10-I2510', '10-I5084', '10-N189', '10-R627', '10-E875', '10-I482', '10-Z7901', '10-L270', '10-I255', '10-G629', '10-D649', '10-R7989', '10-Z66', '10-Z95810', '10-T465X5A', '10-Y92230'], [], []), '2147-01-06'), ((['10-I2109', '10-J9601', '10-I5023', '10-I442', '10-R570', '10-I2510', '10-I255', '10-I482', '10-Z7901', '10-N189', '10-E785', '10-M109', '10-N400', '10-M1990', '10-Z66', '10-E875', '10-D649', '10-R34', '10-I252', '10-I081', '10-Z4502', '10-I2720', '10-Z781'], [], []), '2147-11-19')], '10011938': [((['10-S80212A', '10-R609', '10-R2681', '10-J449', '10-I509', '10-Q282', '10-G40909', '10-Z9981', '10-W19XXXA', '10-Y929'], [], []), '2128-12-23'), ((['10-G40802', '10-L89623', '10-L02214', '10-N390', '10-L03314', '10-B9620', '10-R310', '10-I10', '10-J449', '10-D509', '10-Z87891', '10-L98419'], [], []), '2132-01-30')], '10002869': [((['10-O26892', '10-R002', '10-R0602', '10-O09522', '10-Z3A23', '10-Z8619', '10-O99342', '10-F419'], [], []), '2136-04-06'), ((['10-O34211', '10-D649', '10-O99344', '10-F419', '10-O9902', '10-Z3A39', '10-Z370', '10-Z302'], [], []), '2136-07-27')], '10005858': [((['10-N62', '10-N6082', '10-N6081', '10-E6601', '10-Z6842', '10-E039', '10-E042', '10-G8929', '10-D869', '10-I10', '10-G4733', '10-M4802', '10-K589', '10-M170', '10-M545', '10-F329', '10-M25531', '10-R7303', '10-Z96653', '10-Z8679', '10-Z833', '10-Z8249', '10-Z803'], [], []), '2177-02-11'), ((['10-I5031', '10-Z6843', '10-R079', '10-D649', '10-E669', '10-E039', '10-R413', '10-K219', '10-G2581', '10-G4700', '10-H04129', '10-R3915'], [], []), '2177-07-21')], '10002013': [((['10-R0789', '10-E876', '10-I25119', '10-I25709', '10-Z951', '10-Z955', '10-E1122', '10-E1165', '10-I129', '10-N183', '10-E785', '10-J449', '10-K219', '10-E1140', '10-F329', '10-E669', '10-Z6837', '10-G8929', '10-M25519', '10-G4733', '10-Z794', '10-I252', '10-Z87891'], [], []), '2164-03-19'), ((['10-I25110', '10-E1110', '10-E1122', '10-E11319', '10-M869', '10-I5032', '10-I130', '10-T82855A', '10-E11621', '10-E1142', '10-E1165', '10-E1169', '10-L97529', '10-N183', '10-J449', '10-B961', '10-B951', '10-F329', '10-K219', '10-G4733', '10-Z87891', '10-Z951', '10-Z794', '10-Z9114', '10-G2581', '10-Z955', '10-Y840', '10-Y929'], [], []), '2165-11-26'), ((['10-A419', '10-M86672', '10-M86172', '10-N179', '10-L03116', '10-J9811', '10-I5032', '10-I130', '10-E1152', '10-I96', '10-E1169', '10-L97524', '10-E11621', '10-B9561', '10-J449', '10-E11319', '10-E1142', '10-E1165', '10-I2510', '10-I252', '10-G4733', '10-K219', '10-G2581', '10-N189', '10-E1122', '10-Z794', '10-Z951', '10-Z955', '10-Z87891'], [], []), '2166-04-19'), ((['10-L03031', '10-E1140', '10-Z794', '10-I10', '10-Z89412', '10-E11319', '10-I2510', '10-Z955', '10-K219', '10-J449'], [], []), '2167-07-05')], '10005001': [((['10-D259', '10-E8809', '10-N3289', '10-N8320', '10-N736', '10-Z90721', '10-Z9079', '10-N393', '10-Z975', '10-J309', '10-F329', '10-Z800', '10-Z8042'], [], []), '2164-10-14'), ((['10-C562', '10-C786', '10-C772', '10-C787', '10-E669', '10-F419', '10-F329', '10-Z6836'], [], []), '2165-06-18')], '10010718': [((['10-I481', '10-I5023', '10-N179', '10-I130', '10-I2510', '10-E785', '10-M06042', '10-M06041', '10-M109', '10-M810', '10-E559', '10-D649', '10-I255', '10-M19012', '10-M19011', '10-Z955', '10-Z7902', '10-Z87891'], [], []), '2169-01-27')], '10008816': [((['10-T43621A', '10-T43591A', '10-T528X1A', '10-Y929', '10-R972', '10-F17210'], [], []), '2128-12-19'), ((['10-F10129', '10-I10', '10-R569', '10-K859', '10-R011', '10-F4310', '10-R4182'], [], []), '2129-04-04'), ((['10-F10129', '10-R4182', '10-I10', '10-R569', '10-R011', '10-F4310', '10-F319', '10-F339'], [], []), '2129-04-05'), ((['10-R45851', '10-F10129', '10-F339', '10-M542', '10-G40909', '10-F1421', '10-F1221'], [], []), '2129-05-17'), ((['10-R45851', '10-T50902A', '10-F10129', '10-F339', '10-F419', '10-F319', '10-Y929'], [], []), '2129-05-21'), ((['10-T426X1A', '10-F10129', '10-F329', '10-F17210', '10-Y929'], [], []), '2129-07-09'), ((['10-F10229', '10-S0990XA', '10-F329', '10-F14129', '10-F11129', '10-F1320', '10-F1520', '10-W109XXA', '10-Y929'], [], []), '2129-08-23'), ((['10-F10239', '10-F10229', '10-R072'], [], []), '2130-08-21'), ((['10-F10129', '10-F1910'], [], []), '2130-11-28'), ((['10-F10129', '10-R569'], [], []), '2132-03-15')], '10006825': [((['10-F29', '10-F319'], [], []), '2157-05-28')], '10002443': [((['10-I309', '10-J9602', '10-I314', '10-I480', '10-E119', '10-Z66', '10-I10', '10-M069', '10-E785', '10-Z86718', '10-Z87891'], [], []), '2183-10-20')], '10005866': [((['10-K565', '10-E43', '10-J189', '10-J952', '10-K766', '10-M96831', '10-D62', '10-T814XXA', '10-L03311', '10-K7031', '10-M47817', '10-Z6823', '10-F17210', '10-D696', '10-Y838', '10-Y92239', '10-T17990A', '10-R339'], [], []), '2148-03-21'), ((['10-A084', '10-I81', '10-R188', '10-E872', '10-K766', '10-E871', '10-K7469', '10-K7290', '10-Z720'], [], []), '2149-02-14'), ((['10-K565', '10-K7031', '10-I8510', '10-K766', '10-F17210', '10-Z6823', '10-B1920', '10-I81', '10-E43'], [], []), '2149-06-25'), ((['10-K56600', '10-K766', '10-I8510', '10-K7031', '10-F17210', '10-M47818'], [], []), '2149-09-19'), ((['10-K660', '10-E43', '10-K766', '10-Z720', '10-K7031', '10-Z6820', '10-R1084'], [], []), '2149-09-26'), ((['10-K265', '10-E43', '10-J811', '10-D688', '10-K766', '10-D6959', '10-D62', '10-I471', '10-I959', '10-K255', '10-I4891', '10-K209', '10-B182', '10-K7031', '10-F1011', '10-K660', '10-K3189', '10-Z5309', '10-G8929', '10-I8510', '10-Z66', '10-Z515', '10-R0902', '10-D638', '10-Z590', '10-Z6824', '10-K449', '10-Z720'], [], []), '2149-10-25')], '10004113': [((['10-D1802', '10-I619', '10-G40909'], [], []), '2173-03-22')], '10006640': [((['10-D134', '10-G8918', '10-R112', '10-R0789', '10-Z800', '10-E119', '10-Z794', '10-R748', '10-M797', '10-K589', '10-F419', '10-E669', '10-K219', '10-G4700'], [], []), '2173-12-19')], '10010663': [((['10-J869', '10-J154', '10-J90', '10-E872', '10-J9811', '10-D649', '10-B955', '10-E860', '10-E8339', '10-E8342', '10-E876', '10-E8809', '10-K219'], [], []), '2146-10-13')], '10002528': [((['10-F4489', '10-R29818', '10-I7300', '10-I951', '10-R000', '10-F410', '10-E869', '10-K219', '10-D509', '10-D519', '10-F329'], [], []), '2168-12-20'), ((['10-R29818', '10-F411', '10-F329', '10-F4310', '10-F5082', '10-Z6822', '10-D649'], [], []), '2170-03-18')], '10006431': [((['10-C250', '10-H905', '10-I10', '10-E042', '10-K838', '10-L298', '10-K219', '10-D509', '10-G4700', '10-Z800'], [], []), '2128-01-16'), ((['10-K521', '10-K831', '10-C250', '10-D6959', '10-E860', '10-I10', '10-K219', '10-R110', '10-T451X5A', '10-Y929'], [], []), '2128-03-07'), ((['10-E860', '10-C250', '10-K521', '10-D701', '10-H905', '10-I10', '10-K219', '10-R112', '10-T451X5A', '10-Z6823', '10-R630', '10-E7800', '10-F419', '10-Y929'], [], []), '2128-03-30'), ((['10-K819', '10-K821', '10-C250', '10-E785', '10-I10', '10-K219', '10-K8681'], [], []), '2128-06-12'), ((['10-G893', '10-Z66', '10-K8580', '10-E43', '10-C250', '10-C787', '10-C7900', '10-I959', '10-K219', '10-Z681', '10-I10', '10-E049', '10-E7800', '10-K5900'], [], []), '2129-01-30'), ((['10-C250', '10-C787', '10-E860', '10-H903', '10-K219', '10-I10', '10-D473', '10-D72829', '10-E861', '10-R112', '10-Z66', '10-G893', '10-Z800', '10-R634', '10-Z6821'], [], []), '2129-02-06')], '10004764': [((['10-I2510', '10-I959', '10-I4891', '10-D649', '10-I10', '10-E7800', '10-Z955', '10-Z87891', '10-R0902'], [], []), '2168-04-16')], '10003385': [((['10-K629', '10-L910', '10-I10', '10-D573', '10-E669', '10-Z6831', '10-Z87891'], [], []), '2131-09-26')], '10005123': [((['10-K7460', '10-I81', '10-N179', '10-K766', '10-I8510', '10-K740', '10-D6959', '10-E785', '10-I864'], [], []), '2129-07-05'), ((['10-J95821', '10-I81', '10-T82868A', '10-K766', '10-I8510', '10-K740', '10-I864', '10-D6959', '10-E785', '10-Y838', '10-K7460', '10-Y92238'], [], []), '2129-08-19')], '10003400': [((['10-T8131XA', '10-R6521', '10-J9601', '10-N179', '10-A419', '10-G9340', '10-C9000', '10-J910', '10-K5660', '10-C786', '10-C7889', '10-C211', '10-E46', '10-Q620', '10-T814XXA', '10-T8359XA', '10-N12', '10-B3749', '10-K311', '10-I5032', '10-D62', '10-Z66', '10-Z515', '10-E8809', '10-I482', '10-Z7901', '10-M179', '10-I129', '10-B9562', '10-N183', '10-Z433', '10-R310', '10-E669', '10-Z6838', '10-Y848', '10-Y92239', '10-B965', '10-D696'], [], []), '2137-09-02')], '10008077': [((['10-I714', '10-I482', '10-I10', '10-I2510', '10-M549', '10-Z7901', '10-Z006', '10-Z951', '10-K219', '10-M1990', '10-Z96651', '10-Z87891', '10-E669', '10-Z6837'], [], []), '2189-03-01'), ((['10-I25118', '10-I9751', '10-I25718', '10-N183', '10-I129', '10-I2582', '10-I480', '10-E785', '10-K219', '10-Y658', '10-Y713', '10-Y92238', '10-Z87891', '10-Z951', '10-Z7902'], [], []), '2191-03-07')], '10002807': [((['10-K8590', '10-I2510', '10-I10', '10-E785', '10-K219', '10-K5792', '10-Z951'], [], []), '2152-03-31')], '10002348': [((['10-C7931', '10-G935', '10-G936', '10-G911', '10-C3490', '10-F05', '10-I10', '10-F17210', '10-G510', '10-M21372', '10-E039', '10-M810', '10-Z781', '10-R001', '10-R739', '10-T380X5A', '10-Y92239', '10-D72829', '10-R3915'], [], []), '2112-12-10')], '10008245': [((['10-S32391A', '10-E870', '10-F72', '10-G40909', '10-R509', '10-J45909', '10-M810', '10-R2681', '10-X58XXXA', '10-Y92098', '10-K5900'], [], []), '2174-04-21')], '10006269': [((['10-B003', '10-C20', '10-K626', '10-K2960', '10-K2980', '10-I10', '10-F329', '10-D508'], [], []), '2124-07-05')], '10011607': [((['10-R002', '10-B379', '10-R51', '10-R079', '10-I10', '10-F329'], [], []), '2185-01-27'), ((['10-S12300A', '10-S0990XA', '10-S0512XA', '10-W1809XA', '10-Y929', '10-J45909', '10-I10', '10-E785', '10-R7302', '10-M810', '10-F0390'], [], []), '2185-03-19')], '10003731': [((['10-E6601', '10-Z6843', '10-K449', '10-K219', '10-G4733', '10-K2270', '10-I480', '10-Z7901', '10-I10', '10-E785'], [], []), '2152-04-14')], '10002545': [((['10-O480', '10-O98813', '10-Z3A41', '10-Z370', '10-B951', '10-O701'], [], []), '2158-09-10')], '10002131': [((['10-I82412', '10-I5033', '10-E873', '10-E46', '10-I482', '10-G309', '10-I82432', '10-F0280', '10-I10', '10-E876', '10-M810', '10-Z66', '10-Z515', '10-Z993', '10-H9192', '10-H409', '10-M25551', '10-K5900', '10-Z6827', '10-Z85828'], [], []), '2128-03-19')], '10003637': [((['10-I429', '10-I2510', '10-I10', '10-Z951', '10-Z955', '10-I252', '10-Z7901', '10-Z8673'], [], []), '2145-11-24'), ((['10-K611', '10-I2582', '10-I509', '10-I2510', '10-I252', '10-Z950', '10-Z951', '10-Z8673', '10-E785', '10-I255', '10-F17210'], [], []), '2146-01-26'), ((['10-N179', '10-I5022', '10-I959', '10-E8339', '10-I2510', '10-Z951', '10-Z950', '10-Z8673', '10-D649', '10-F17210', '10-I10', '10-E785', '10-I255'], [], []), '2146-02-19'), ((['10-K603', '10-I130', '10-I5022', '10-N179', '10-N183', '10-E785', '10-I2510', '10-I480', '10-Z7682', '10-I255', '10-R590', '10-I2722', '10-I252', '10-Z7901', '10-Z8673', '10-Z95810', '10-Z951', '10-Z955', '10-Z87891'], [], []), '2149-05-20')], '10002221': [((['10-M1712', '10-D6861', '10-J449', '10-G4733', '10-F329', '10-E119', '10-M4696', '10-E785', '10-K219', '10-E669', '10-Z7901', '10-Z87891', '10-Z6837', '10-Z86711'], [], []), '2203-06-16'), ((['10-L7632', '10-C50912', '10-D6861', '10-K760', '10-K219', '10-G4733', '10-E785', '10-I671', '10-J449', '10-E119', '10-Y838', '10-F329', '10-Z7901', '10-Z86711', '10-Z87891', '10-Z803', '10-Z8041', '10-Z9012'], [], []), '2204-06-30'), ((['10-M7061', '10-D6861', '10-D509', '10-E559', '10-Z87891', '10-Z86711', '10-Z7901', '10-C50912', '10-G4733', '10-E785', '10-E119', '10-K219', '10-K5790', '10-F329', '10-J449', '10-I83811'], [], []), '2204-07-06'), ((['10-M5116', '10-D6861', '10-N390', '10-D62', '10-I83811', '10-D509', '10-Z87891', '10-Z86711', '10-Z7901', '10-E559', '10-C50912', '10-E785', '10-G4733', '10-E119', '10-K219', '10-K5790', '10-F329', '10-J449', '10-I671', '10-M48061', '10-K5900', '10-E876'], [], []), '2204-07-25')], '10006457': [((['10-N179', '10-I69351', '10-E1121', '10-E1140', '10-E11649', '10-E113393', '10-I10', '10-E785', '10-D649', '10-F17210', '10-Z7984', '10-N141', '10-T39395A', '10-Y929'], [], []), '2151-11-08'), ((['10-R51', '10-R112', '10-N390', '10-E860', '10-R944', '10-E785', '10-I10', '10-E11319', '10-E1142', '10-R809', '10-E538', '10-Z87891', '10-I69351', '10-I69398'], [], []), '2152-12-29'), ((['10-E1165', '10-R109', '10-R112', '10-R42', '10-E1122', '10-I129', '10-N189', '10-Z8673', '10-E872'], [], []), '2154-01-29')], '10001843': [((['10-T82855A', '10-I5031', '10-I2510', '10-Y840', '10-Y929', '10-I252', '10-I110', '10-E785', '10-E119', '10-Z794', '10-I4891', '10-Z7901', '10-E669', '10-Z6831', '10-K219'], [], []), '2131-11-11')], '10004606': [((['10-G40409', '10-K8510', '10-G9340', '10-K8064', '10-E871', '10-I701', '10-I10', '10-I160', '10-K219', '10-I739', '10-E785', '10-F17210', '10-Z95820', '10-Z86718', '10-Z8673'], [], []), '2159-03-06'), ((['10-N390', '10-K2971', '10-N179', '10-F05', '10-I701', '10-G40409', '10-I10', '10-D500', '10-I739', '10-Z720', '10-K219', '10-B9620', '10-K5903', '10-T402X5A', '10-Y92230', '10-Z86718'], [], []), '2159-03-22'), ((['10-K31811', '10-E440', '10-R569', '10-D62', '10-I150', '10-Z681', '10-N390', '10-E785', '10-I739', '10-I69398', '10-R531', '10-R2681', '10-R51', '10-R001', '10-T448X5A', '10-Y92230', '10-R109', '10-F1011', '10-Z86718', '10-Z87891'], [], []), '2159-04-11'), ((['10-K31811', '10-B1910', '10-S0990XA', '10-G629', '10-D62', '10-F1120', '10-I452', '10-I6523', '10-G40909', '10-I951', '10-F319', '10-Q2733', '10-I10', '10-W01198A', '10-Y92008', '10-I701', '10-M5416', '10-E039', '10-E785', '10-J449', '10-K219', '10-Z86718', '10-Z87891', '10-K2270', '10-R110', '10-T402X5A', '10-Y929', '10-I739', '10-I69398', '10-R531', '10-R42', '10-N3090', '10-R079', '10-I459', '10-K5900'], [], []), '2159-09-22')], '10005909': [((['10-I82422', '10-I871', '10-I9751', '10-Y838', '10-Y92234'], [], []), '2144-11-02')], '10011259': [((['10-F329', '10-F419', '10-R45851', '10-G4700'], [], []), '2161-07-03'), ((['10-F339', '10-R45851', '10-I10', '10-F42', '10-F419', '10-F459', '10-Z9049', '10-J45909', '10-G4700', '10-E039', '10-E663', '10-Z6837'], [], []), '2161-07-08')], '10011427': [((['10-K7040', '10-N179', '10-K766', '10-N390', '10-K9181', '10-N281', '10-K7031', '10-I129', '10-N189', '10-B9620', '10-E869', '10-E785', '10-K219', '10-Y848', '10-Y92530'], [], []), '2135-12-26'), ((['10-K767', '10-J690', '10-E43', '10-A419', '10-R6521', '10-E872', '10-N390', '10-E871', '10-N179', '10-K7040', '10-N189', '10-I129', '10-K7031', '10-K219', '10-B9562', '10-M109', '10-D6959', '10-D631', '10-D539', '10-E785', '10-F1021', '10-Z6823'], [], []), '2136-02-12'), ((['10-K7040', '10-N186', '10-K767', '10-I120', '10-E440', '10-K7031', '10-M109', '10-K219', '10-D6959', '10-D631', '10-F1010', '10-Z992', '10-Z6825'], [], []), '2136-03-17'), ((['10-T80211A', '10-A4181', '10-R6521', '10-N186', '10-K767', '10-G9341', '10-T8242XA', '10-Z1622', '10-D684', '10-E871', '10-I120', '10-E440', '10-D61818', '10-Y718', '10-Y929', '10-Z992', '10-K7031', '10-D638', '10-D696', '10-E559', '10-E785', '10-Z6823', '10-K219', '10-M109', '10-K668', '10-I9589', '10-Z781', '10-E876', '10-K7040'], [], []), '2136-04-08')], '10010655': [((['10-F329', '10-R45851', '10-F4310', '10-Z915', '10-Z62810'], [], []), '2167-03-08'), ((['10-F338', '10-F4310', '10-F419'], [], []), '2167-03-21'), ((['10-F339', '10-F4310', '10-Z915', '10-R45851'], [], []), '2167-05-11'), ((['10-F4310', '10-R451', '10-F329', '10-Z915', '10-S50812A', '10-S50811A', '10-X789XXA', '10-Y929'], [], []), '2167-07-14'), ((['10-F329', '10-S61512A', '10-S61511A', '10-X788XXA', '10-Y929', '10-F4310', '10-F449', '10-F603'], [], []), '2167-07-27'), ((['10-F329', '10-R45851', '10-F4310', '10-N390', '10-J111', '10-F419', '10-F603'], [], []), '2167-08-29'), ((['10-J111', '10-R45851', '10-F4310', '10-F603', '10-L309', '10-F329', '10-F39'], [], []), '2167-09-01'), ((['10-F329', '10-R45851', '10-F4310', '10-F603'], [], []), '2168-03-29')], '10011668': [((['10-I214', '10-I110', '10-I5023', '10-I428', '10-R0789', '10-E785', '10-R0902', '10-F329', '10-R300', '10-R9431', '10-E806', '10-R791', '10-Z7901', '10-Z952', '10-Z87891'], [], []), '2141-04-20')], '10008819': [((['10-K436', '10-I10', '10-F17210'], [], []), '2169-08-25')], '10003757': [((['10-S066X0A', '10-G92', '10-N390', '10-W050XXA', '10-Y92009', '10-I2510', '10-I10', '10-I739', '10-J439', '10-Z96641', '10-B9620', '10-F0390', '10-I350', '10-D649', '10-D469', '10-S0181XA', '10-M4854XD', '10-J8410', '10-Z66', '10-M8448XD', '10-S7002XA', '10-R402142', '10-R402362', '10-R402252'], [], []), '2142-09-15')], '10011449': [((['10-L03115', '10-B955', '10-E039', '10-L309'], [], []), '2135-12-01')], '10012206': [((['10-K8510', '10-E43', '10-N179', '10-D684', '10-I82890', '10-K8010', '10-E861', '10-K269', '10-I10', '10-G4733', '10-E1140', '10-Z7984', '10-F17210', '10-G250', '10-E8342', '10-D6959', '10-E781', '10-Z6822', '10-B957'], [], []), '2127-07-14')], '10011279': [((['10-K047', '10-F17210'], [], []), '2179-05-03')], '10000117': [((['10-R1310', '10-R0989', '10-K31819', '10-K219', '10-K449', '10-F419', '10-I341', '10-M810', '10-Z87891'], [], []), '2181-11-15'), ((['10-S72012A', '10-W010XXA', '10-Y93K1', '10-Y92480', '10-K219', '10-E7800', '10-I341', '10-G43909', '10-Z87891', '10-Z87442', '10-F419', '10-M810', '10-Z7901'], [], []), '2183-09-21')], '10001884': [((['10-J441', '10-F17210', '10-R079'], [], []), '2130-06-24'), ((['10-K921', '10-D62', '10-I4891', '10-J449', '10-I10', '10-F17210', '10-D649', '10-I2510', '10-F419', '10-E785', '10-I739', '10-M47892', '10-R0602', '10-G4700', '10-H409', '10-K449', '10-K2970', '10-Z7982', '10-Z7901', '10-Z96649'], [], []), '2130-08-23'), ((['10-J441', '10-R0902', '10-E876', '10-I10', '10-E780', '10-E785', '10-Z87891', '10-Z9981'], [], []), '2130-10-06'), ((['10-I4891', '10-J441', '10-Z9981', '10-Z7901', '10-I10', '10-E785', '10-F419', '10-I739', '10-I2510', '10-Z87891', '10-D509', '10-R079', '10-G4700', '10-H409'], [], []), '2130-10-12'), ((['10-J441', '10-Z9981', '10-I10', '10-I4891', '10-Z7901', '10-Z87891'], [], []), '2130-10-14'), ((['10-J441', '10-I4892', '10-Z9981', '10-I480', '10-J45998', '10-Z87891', '10-I2510', '10-Z96649', '10-I10', '10-E785', '10-D509', '10-I739', '10-F419', '10-K5900', '10-M1990', '10-Z825', '10-Z8249', '10-Z7901'], [], []), '2130-10-22'), ((['10-J441', '10-M7989', '10-I2510', '10-I499', '10-I739', '10-F419', '10-Z87891', '10-E876'], [], []), '2130-11-20'), ((['10-J441', '10-I4892', '10-I248', '10-J45909', '10-Z87891', '10-I4891', '10-F419', '10-G4700', '10-E780', '10-I2510', '10-E876', '10-R312', '10-I739', '10-Z7952', '10-Z9981'], [], []), '2130-11-30'), ((['10-J441', '10-Z9981', '10-I4891', '10-J45909', '10-Z7901', '10-I10', '10-I2510', '10-E785', '10-M1990', '10-F419', '10-I739', '10-G4700', '10-D649', '10-Z96649', '10-Z87891'], [], []), '2130-12-08'), ((['10-J441', '10-J45909', '10-H6991', '10-F419', '10-G4700', '10-I4891', '10-Z7901', '10-I10', '10-I2510', '10-D649', '10-E785', '10-Z87891', '10-Z006'], [], []), '2130-12-24'), ((['10-J441', '10-N179', '10-Z9981', '10-I4891', '10-D649', '10-I10', '10-E785', '10-G5622', '10-I2510', '10-M1990', '10-Z96649', '10-Z87891', '10-J45909', '10-F419', '10-G4700', '10-R040', '10-I739'], [], []), '2130-12-30'), ((['10-J441', '10-K7200', '10-R579', '10-J9602', '10-J9601', '10-I442', '10-I82621', '10-I4891', '10-D696', '10-I469', '10-I10', '10-E785', '10-Z7901', '10-M47892', '10-I7389', '10-I2510', '10-Z87891', '10-Z96641', '10-J0190', '10-F419', '10-G4700', '10-R609', '10-D509', '10-R509', '10-M1990', '10-Y92239', '10-Z7952', '10-Z825', '10-Z781', '10-Z515', '10-Z66', '10-Z9981', '10-J45909', '10-I447', '10-M7981', '10-T45515A'], [], []), '2131-01-20')], '10001667': [((['10-R471', '10-I5030', '10-E538', '10-Z66', '10-I4891', '10-Z7902', '10-I110', '10-E7849', '10-K5790', '10-Z87891', '10-R946', '10-I455', '10-R413'], [], []), '2173-08-24')], '10004720': [((['10-J690', '10-G931', '10-I82621', '10-Z8674', '10-J95811', '10-I959', '10-J9600', '10-I10', '10-D638', '10-Y848', '10-Y92238', '10-Z66', '10-Z515', '10-Z781', '10-F209', '10-S42291D', '10-W1830XD', '10-Z9181', '10-Z85828', '10-H269', '10-Z720', '10-L409', '10-M810', '10-L570', '10-M720', '10-J309', '10-F1021', '10-Z7902', '10-H6120'], [], []), '2186-11-17')], '10011189': [((['10-R55', '10-H539', '10-H9319', '10-I10', '10-K625', '10-I720', '10-L409', '10-K219', '10-I951'], [], []), '2188-02-26'), ((['10-I471', '10-I671', '10-I10', '10-I2510', '10-K219', '10-I951', '10-D649', '10-Z598', '10-E785'], [], []), '2188-03-25'), ((['10-I471', '10-E785', '10-I959', '10-I10', '10-Z8711', '10-I2510', '10-Z8673', '10-L409'], [], []), '2189-07-04')], '10004322': [((['10-J189', '10-F200', '10-N179', '10-E1143', '10-J449', '10-Z794', '10-Z87891', '10-I951', '10-K219', '10-I2510', '10-I10', '10-E785', '10-Z9181'], [], []), '2134-10-02'), ((['10-N179', '10-R339', '10-E860', '10-F200', '10-I10', '10-E119', '10-F88', '10-K219', '10-E7800', '10-J449', '10-J45909', '10-Z794'], [], []), '2135-01-03'), ((['10-T83511A', '10-G92', '10-I10', '10-B9620', '10-J449', '10-E119', '10-F200', '10-N390', '10-Y846', '10-Y929', '10-Z87891', '10-K210', '10-I2510', '10-E785', '10-D649'], [], []), '2135-02-12')], '10001919': [((['10-C169', '10-C786', '10-I82621', '10-K9171', '10-Y838', '10-Y92234', '10-N359', '10-E039', '10-K219', '10-Z7901', '10-Z8546'], [], []), '2124-04-21')], '10006630': [((['10-F329', '10-F419', '10-R45851', '10-F909'], [], []), '2177-03-01')], '10010997': [((['10-T814XXA', '10-L0889', '10-B952', '10-B9561', '10-Y838', '10-Y92018', '10-L608', '10-M069', '10-I10', '10-K219', '10-L820', '10-M6289'], [], []), '2139-05-02')], '10006029': [((['10-K831', '10-I81', '10-C7800', '10-K862', '10-C771', '10-E785', '10-I129', '10-E1122', '10-N189', '10-F419', '10-N400', '10-D539', '10-F329', '10-E039', '10-Z85528', '10-Z905', '10-Z87891', '10-Z800', '10-Z807', '10-Z8041'], [], []), '2169-08-22'), ((['10-A408', '10-K831', '10-I81', '10-J189', '10-K830', '10-C7802', '10-C7801', '10-C771', '10-N179', '10-C249', '10-R6520', '10-F419', '10-N400', '10-Z85528', '10-E785', '10-Z87891', '10-D539', '10-F329', '10-E039', '10-E1122', '10-I129', '10-N189', '10-Z794'], [], []), '2169-10-05'), ((['10-N179', '10-C221', '10-C259', '10-C7802', '10-C7801', '10-I10', '10-E785', '10-F419', '10-N400', '10-I129', '10-E1122', '10-N189', '10-Z7984', '10-Z87891', '10-T464X5A', '10-Y929', '10-D6481', '10-T451X5A', '10-E039', '10-Z794', '10-Z905', '10-I959', '10-E860', '10-Z7901'], [], []), '2170-07-12'), ((['10-C786', '10-E43', '10-C250', '10-C787', '10-C781', '10-C771', '10-N179', '10-E871', '10-G893', '10-K8681', '10-Z85528', '10-Z8589', '10-Z905', '10-Z66', '10-Z515', '10-E1122', '10-I129', '10-N189', '10-Z794', '10-E7849', '10-N400', '10-K529', '10-Z87891', '10-K117', '10-D630', '10-E860', '10-Z6826', '10-Z7902', '10-B001', '10-F419', '10-G4700', '10-R739', '10-T380X5A', '10-Y92230', '10-K5900'], [], []), '2170-08-11')], '10010867': [((['10-S12600A', '10-S271XXA', '10-S066X0A', '10-J9600', '10-S240XXA', '10-F05', '10-S22049A', '10-S22059A', '10-S22069A', '10-S32029A', '10-Z6841', '10-D62', '10-J95851', '10-R339', '10-F10129', '10-F1290', '10-E669', '10-F329', '10-Z781', '10-V270XXA', '10-Y929', '10-S8012XA', '10-Z23', '10-Y848', '10-Y92239', '10-F419'], [], []), '2148-01-11'), ((['10-S22059A', '10-F329', '10-S22069A', '10-V499XXA', '10-Y929', '10-E669', '10-Z6833', '10-Z981', '10-J45909', '10-G43909', '10-F1910', '10-M40209'], [], []), '2148-01-30'), ((['10-J90', '10-I272', '10-M25511', '10-Z981', '10-Z87820', '10-V8609XS', '10-Y92488', '10-J45909', '10-E041', '10-G43909', '10-F1210'], [], []), '2148-03-13')], '10002266': [((['10-O4202', '10-O411230', '10-M358', '10-O621', '10-O631', '10-Z3A39', '10-Z370', '10-R760', '10-O99284', '10-E039', '10-I7300'], [], []), '2131-03-06')], '10005749': [((['10-E1165', '10-N179', '10-E1122', '10-I130', '10-N189', '10-I5022', '10-N390', '10-E871', '10-E876', '10-I482', '10-F419', '10-Z940', '10-Z794', '10-Z7901', '10-E785'], [], []), '2145-09-14')], '10002495': [((['10-I214', '10-R570', '10-I509', '10-R578', '10-A047', '10-N179', '10-S3730XA', '10-I2510', '10-E118', '10-X58XXXA', '10-Y92239', '10-I10', '10-E785', '10-T45515A', '10-Z86718', '10-Z7901', '10-R310', '10-Z7902', '10-I480', '10-Z23', '10-K2960', '10-B9681', '10-Z87891', '10-I252', '10-R410', '10-K219'], [], []), '2141-05-29')], '10007232': [((['10-O701', '10-Z370', '10-Z3A39'], [], []), '2120-06-07')], '10010038': [((['10-J029', '10-J36', '10-R509'], [], []), '2167-03-18')], '10011912': [((['10-S82142A', '10-B1920', '10-C50919', '10-W03XXXA', '10-Y92520', '10-J45909', '10-K7030', '10-E785', '10-F329', '10-M810', '10-F17200', '10-F1021'], [], []), '2176-10-21'), ((['10-M4854XA', '10-R030', '10-F329', '10-M810', '10-E785', '10-B1920', '10-K7460', '10-J45909'], [], []), '2177-10-05'), ((['10-R627', '10-E43', '10-Z681', '10-A048', '10-K7030', '10-B1920', '10-F1011', '10-Z853', '10-K2970', '10-F1911', '10-F1290', '10-J45909', '10-E785', '10-M810', '10-I10', '10-Z880', '10-Z882', '10-Z888', '10-K529', '10-G4700', '10-R000', '10-H53141', '10-R51', '10-L539', '10-M549', '10-Z87311', '10-K5900'], [], []), '2179-09-25')], '10007058': [((['10-I214', '10-I7102', '10-K219', '10-Z23', '10-Z7902', '10-Z7982'], [], []), '2167-11-11')], '10003019': [((['10-I340', '10-I5032', '10-Z006', '10-D8689', '10-E785', '10-J45909', '10-K219', '10-N400', '10-Z8042', '10-G4733', '10-Z8572', '10-Z87891', '10-Z9621', '10-H9190'], [], []), '2181-03-04')], '10004296': [((['10-O133', '10-O722', '10-O8612', '10-O639', '10-L03311', '10-L02211', '10-O324XX0', '10-O99334', '10-O9952', '10-R609', '10-J45909', '10-O860', '10-Z3A37', '10-Z370'], [], []), '2168-11-08')], '10002976': [((['10-E1065', '10-Z794', '10-D72829', '10-E860', '10-I2510', '10-Z955', '10-E1042', '10-E103592', '10-E785', '10-I252'], [], []), '2153-08-14')], '10010848': [((['10-R51', '10-A812', '10-B20', '10-I10', '10-E785', '10-Z87891', '10-Z8673'], [], []), '2169-01-28')], '10006513': [((['10-N136', '10-B9620', '10-N179', '10-E119', '10-Z794'], [], []), '2125-05-07'), ((['10-M10072', '10-N179', '10-M10062', '10-E1122', '10-N189', '10-E860', '10-Z87442', '10-Z794'], [], []), '2127-03-28')], '10001401': [((['10-C675', '10-I10', '10-D259', '10-Z87891', '10-E785', '10-E890'], [], []), '2131-06-15'), ((['10-T814XXA', '10-K651', '10-N179', '10-I82412', '10-C679', '10-I10', '10-B966', '10-R7881', '10-Y838', '10-Y9289', '10-F17210', '10-Z436', '10-Z90710', '10-D72829', '10-Z96652'], [], []), '2131-07-02'), ((['10-I2699', '10-I82412', '10-N390', '10-I471', '10-I10', '10-I872', '10-R918', '10-B952', '10-E039', '10-E785', '10-E876', '10-E8342', '10-G4700', '10-K5900', '10-Z66', '10-N63', '10-D509', '10-D638', '10-Z7901', '10-Z8551', '10-Z906', '10-Z87891', '10-Z96652'], [], []), '2131-08-04'), ((['10-T814XXA', '10-A419', '10-K651', '10-N179', '10-N1330', '10-D62', '10-I2782', '10-N138', '10-C679', '10-I10', '10-E785', '10-E039', '10-K439', '10-K435', '10-E876', '10-Y838', '10-Y929', '10-Z96652', '10-Z86718', '10-N63', '10-Z7901', '10-Z87891'], [], []), '2131-10-05'), ((['10-N99820', '10-E43', '10-R310', '10-N131', '10-D62', '10-R8271', '10-E039', '10-E785', '10-N9989', '10-I10', '10-Z86718', '10-Z936', '10-Z7902', '10-Z86711', '10-Z87891', '10-Z6822', '10-Z8551', '10-Z96652', '10-Y848', '10-Y833', '10-Y929'], [], []), '2131-11-15'), ((['10-T8140XA', '10-A4181', '10-R6520', '10-N179', '10-N1330', '10-N12', '10-T8144XA', '10-Z936', '10-I10', '10-E785', '10-E039', '10-Z87891', '10-Z8551', '10-Z86718', '10-Y848', '10-Y92239'], [], []), '2133-07-13')], '10008742': [((['10-I472', '10-F319', '10-R0600', '10-F4324', '10-F17210', '10-F1010'], [], []), '2173-06-26')], '10011365': [((['10-F322', '10-I69351', '10-R45851', '10-N390', '10-Z681', '10-I10', '10-N3941', '10-I69391', '10-R1310', '10-E7800', '10-I252', '10-I2510', '10-I69322', '10-R634'], [], []), '2166-01-28'), ((['10-A419', '10-J9601', '10-J690', '10-R6521', '10-J189', '10-R64', '10-R1312', '10-I69351', '10-I4891', '10-N390', '10-Z681', '10-Z66', '10-Z515', '10-Z781', '10-T18128A', '10-X58XXXA', '10-Y929', '10-I69391', '10-I10', '10-I2510', '10-E785', '10-F329', '10-I447', '10-I69322', '10-I7300', '10-E7800', '10-K5900', '10-I69398', '10-R2689', '10-Z87891', '10-B9620', '10-N3941'], [], []), '2166-02-20')], '10007134': [((['10-S270XXA', '10-S2242XA', '10-Y33XXXA', '10-Y92481', '10-Z590', '10-F1010', '10-F1210', '10-Z720'], [], []), '2140-05-24')], '10005606': [((['10-S12390A', '10-J9690', '10-S27321A', '10-S14109A', '10-S2220XA', '10-F10121', '10-A0472', '10-G960', '10-S22020A', '10-S12490A', '10-S12590A', '10-S12690A', '10-S93409A', '10-W109XXA', '10-Y9289', '10-S0101XA', '10-F17210', '10-F1290', '10-D649', '10-M5382'], [], []), '2143-12-16'), ((['10-K920', '10-N179', '10-F1010', '10-I10', '10-F17200', '10-E8352', '10-E876', '10-E8342', '10-K7030', '10-D6959'], [], []), '2144-03-08')]}\n",
            "len test: 20\n",
            "Test dataset: {'10003502': [((['10-R001', '10-I5033', '10-F0391', '10-I2582', '10-E871', '10-Z9181', '10-I2510', '10-I252', '10-I10', '10-I350', '10-I4891', '10-Z7902', '10-E785', '10-Z66'], [], []), '2169-08-28')], '10002930': [((['10-F10239', '10-F1110', '10-R45851', '10-Z87820', '10-B1920', '10-Z23', '10-Z590', '10-F1410', '10-R509', '10-F29', '10-Z21', '10-Z9114', '10-D72819'], [], []), '2198-04-22'), ((['10-F1994', '10-B20', '10-F1490', '10-F1099', '10-F209', '10-Z87820', '10-B1920', '10-Z87891'], [], []), '2198-05-04'), ((['10-F29', '10-S0990XA', '10-F1910', '10-N390', '10-F1010', '10-F39', '10-F17200', '10-Z87820', '10-Z590', '10-Z21', '10-W19XXXA', '10-Y929'], [], []), '2199-02-19'), ((['10-E162', '10-F10129', '10-Z21', '10-B1920', '10-R4182'], [], []), '2200-06-05'), ((['10-F10129', '10-R4182', '10-E162', '10-B20', '10-B1920'], [], []), '2201-02-13'), ((['10-F39', '10-R45851', '10-M542', '10-B20', '10-Z9114', '10-F17210', '10-R51', '10-W19XXXA', '10-Y929', '10-B1920', '10-S0990XA', '10-D696'], [], []), '2201-03-26')], '10009129': [((['10-S68522A', '10-B182', '10-W312XXA', '10-Y9269', '10-F1290'], [], []), '2183-10-23')], '10000084': [((['10-G3183', '10-F0280', '10-R441', '10-R296', '10-E785', '10-Z8546'], [], []), '2160-11-25'), ((['10-R4182', '10-G20', '10-F0280', '10-R609', '10-E785', '10-Z8546'], [], []), '2160-12-28')], '10010471': [((['10-I4891', '10-N186', '10-J189', '10-Z7682', '10-I248', '10-J449', '10-Z992', '10-I2510', '10-I350', '10-I340', '10-D631', '10-E039', '10-M5489', '10-M25512', '10-Z79891', '10-G8929'], [], []), '2155-05-10'), ((['10-I214', '10-N186', '10-I462', '10-J9690', '10-I5033', '10-S2243XA', '10-I959', '10-R7881', '10-I132', '10-I480', '10-Z9981', '10-J449', '10-I080', '10-I447', '10-E785', '10-I2510', '10-E039', '10-M5489', '10-Z87891', '10-K219', '10-Z45018', '10-D631', '10-Z992', '10-Y848', '10-Y9289', '10-R740', '10-Z515', '10-M7981', '10-T45515A', '10-Y92239'], [], []), '2155-12-07')], '10005236': [((['10-S72001A', '10-E119', '10-J449', '10-I10', '10-W1839XA', '10-Y92009', '10-Z87891', '10-I70209', '10-Z23'], [], []), '2180-07-03')], '10002315': [((['10-F329', '10-R45851'], [], []), '2161-03-24')], '10003372': [((['10-D497', '10-K740', '10-I10', '10-K219', '10-K660', '10-Z87891'], [], []), '2185-07-08')], '10010993': [((['10-S02652A', '10-S2242XA', '10-Y09', '10-Y929', '10-Z23', '10-Z87891'], [], []), '2116-05-15')], '10004719': [((['10-T82868A', '10-I82441', '10-Y832', '10-Y92009', '10-I70411', '10-Z86718', '10-J45909'], [], []), '2183-09-03')], '10010231': [((['10-C9200', '10-E854', '10-D701', '10-K1230', '10-L99', '10-K219', '10-R0781', '10-R42', '10-R300', '10-R197', '10-T451X5A', '10-R5081', '10-Y92239'], [], []), '2117-12-05'), ((['10-Z5111', '10-C9200'], [], []), '2117-12-23'), ((['10-D709', '10-C9200', '10-R5081', '10-R1011', '10-M79662', '10-M79661', '10-L729'], [], []), '2118-01-08'), ((['10-Z5111', '10-C9200', '10-Z91048'], [], []), '2118-01-20'), ((['10-D701', '10-C9200', '10-T451X5A', '10-R5081', '10-Y92009', '10-D61818'], [], []), '2118-02-07'), ((['10-Z5111', '10-D61810', '10-C92Z0', '10-R740', '10-K5903', '10-T451X5A', '10-Y92230'], [], []), '2118-02-26'), ((['10-K2960', '10-D61810', '10-C92Z0', '10-K219', '10-E860', '10-T451X5A', '10-Y9289', '10-Z79899'], [], []), '2118-03-09'), ((['10-Z5111', '10-C92Z0', '10-K760', '10-K219', '10-K2970'], [], []), '2118-04-07'), ((['10-A4101', '10-D61810', '10-L03221', '10-E871', '10-C92Z1', '10-L731', '10-R5081', '10-T451X5A', '10-Y92009', '10-R918', '10-R740', '10-T50995A', '10-Y92230'], [], []), '2118-05-09')], '10003299': [((['10-I639', '10-G8194', '10-E119', '10-I672', '10-Z8673', '10-I10', '10-E785', '10-I252', '10-Z85038', '10-E538', '10-Z9181', '10-F17210', '10-I6529', '10-I2510', '10-R32', '10-R159', '10-M8580', '10-Z7902'], [], []), '2181-10-23'), ((['10-R042', '10-E210', '10-R918', '10-Z8673', '10-E119', '10-E785', '10-F17210', '10-I252', '10-Z85038', '10-J9819'], [], []), '2183-02-28'), ((['10-E876', '10-C3490', '10-J029', '10-Z8673', '10-I10', '10-E119', '10-Z85038', '10-F17210', '10-E785'], [], []), '2183-06-19'), ((['10-K208', '10-E870', '10-C342', '10-E46', '10-Y842', '10-Y92009', '10-D519', '10-E876', '10-R197', '10-F17210', '10-Z85038', '10-E119', '10-Z8673', '10-E785', '10-Z6822', '10-R32', '10-R1310'], [], []), '2183-07-01'), ((['10-K208', '10-E43', '10-I82C11', '10-C342', '10-D6869', '10-E512', '10-I69354', '10-R627', '10-K2950', '10-B9681', '10-E860', '10-K222', '10-I10', '10-K224', '10-K449', '10-K2980', '10-F17210', '10-Z66', '10-Z6822', '10-Y842', '10-Y92009', '10-Z9221', '10-Z7901', '10-Z85038', '10-Z9049'], [], []), '2183-08-01')], '10010393': [((['10-R338', '10-A084', '10-T398X5A', '10-G588', '10-E039', '10-F450', '10-M545', '10-I951', '10-Z9689', '10-Y929'], [], []), '2136-07-02'), ((['10-G588', '10-I10', '10-R338', '10-R202', '10-Z9689', '10-G43909', '10-E039', '10-F459', '10-H6691', '10-I951', '10-K219', '10-K5900'], [], []), '2136-07-07'), ((['10-G588', '10-E039', '10-F419', '10-K828', '10-I498', '10-K219', '10-Z9689', '10-F450'], [], []), '2136-11-13')], '10004457': [((['10-I6521', '10-I2510', '10-Z955', '10-Z8546', '10-Z8571', '10-E785', '10-Z951', '10-I081', '10-Z952', '10-G4733'], [], []), '2147-12-21'), ((['10-I25110', '10-Z951', '10-Z955', '10-I110', '10-I5022', '10-E785', '10-Z8673', '10-Z7902', '10-R21', '10-J45909', '10-I4892', '10-I6522', '10-Z952', '10-Z8571', '10-Z8546', '10-Z87891', '10-Z8249'], [], []), '2148-09-15')], '10002428': [((['10-K922', '10-E43', '10-M8008XA', '10-F0390', '10-Z681', '10-R000', '10-I10', '10-F329', '10-E039', '10-Z66', '10-Z931', '10-M3500', '10-Z5309', '10-I340'], [], []), '2160-04-18'), ((['10-S0990XA', '10-S4992XA', '10-S79912A', '10-W1839XA', '10-Z9181', '10-Y92129', '10-M3500', '10-H53143', '10-H04123', '10-H3530', '10-Z961', '10-E039', '10-M4856XA', '10-I10', '10-M5030', '10-E049'], [], []), '2160-07-16')], '10002430': [((['10-K4030', '10-I480', '10-I272', '10-I509', '10-I2510', '10-I10', '10-K219', '10-N400', '10-E785', '10-J439', '10-I4510', '10-Z7982', '10-Z951', '10-Z87891'], [], []), '2125-06-25'), ((['10-I5033', '10-I281', '10-I272', '10-I480', '10-E871', '10-J449', '10-I2781', '10-I2510', '10-K219', '10-I10', '10-N400', '10-E785', '10-Z902', '10-Z7982', '10-Z9861', '10-Z87891'], [], []), '2125-09-30'), ((['10-I130', '10-I5023', '10-N179', '10-C799', '10-N183', '10-Z87891', '10-J439', '10-Z951', '10-Z955', '10-I071', '10-I2720', '10-I480', '10-Z7902', '10-Z8673', '10-C439', '10-I6523', '10-K219', '10-I4510', '10-N400', '10-E7849', '10-Z902', '10-R740', '10-I255'], [], []), '2129-05-02'), ((['10-I5023', '10-J189', '10-J9691', '10-C7951', '10-J90', '10-N179', '10-I255', '10-I480', '10-Z7902', '10-I2720', '10-I079', '10-I2510', '10-Z951', '10-N183', '10-D638', '10-D696', '10-C439', '10-Z87891'], [], []), '2129-06-24')], '10008647': [((['10-S065X9A', '10-S0081XA', '10-W19XXXA', '10-Y92009', '10-R55', '10-I10', '10-M542', '10-R531'], [], []), '2118-04-01')], '10003412': [((['10-M4856XA', '10-K913', '10-T8489XA', '10-M5136', '10-Y831', '10-Y92239', '10-Y838', '10-Y92009'], [], []), '2173-01-20')], '10007818': [((['10-K7469', '10-K767', '10-J9600', '10-A4151', '10-N179', '10-K661', '10-R571', '10-R6521', '10-K659', '10-N183', '10-E43', '10-K7201', '10-T8119XA', '10-K7200', '10-D684', '10-J95851', '10-R188', '10-D62', '10-I4892', '10-K766', '10-E871', '10-N390', '10-K567', '10-E872', '10-J9811', '10-I97610', '10-T80219A', '10-K7581', '10-I129', '10-D638', '10-I25118', '10-Z955', '10-E119', '10-Z794', '10-R627', '10-I4891', '10-B9689', '10-B9561', '10-Y848'], [], []), '2146-07-12')], '10010150': [((['10-O701', '10-Z370', '10-D573', '10-Z23', '10-O9902', '10-Z3A38'], [], []), '2121-04-05')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "\n",
        "TransformEHR uses an encoderdecoder transformer architecture. The encoder processes the input embeddings and generates a set of hidden representations for each predictor. TransformEHR performs cross-attention over the hidden representations from the encoder and assigns an attention weight for each\n",
        "representation. These weighted representations are then fed to the decoder, which generates ICD codes of the future visit. The decoder generates ICDcodes following the sequential order of code priority within a visit.\n",
        "\n",
        "We are using the existing code for defining the Model architecture - https://github.com/whaleloops/TransformEHR/blob/main/icdmodelbart.py. We made sure the code is updated with no compilation errors."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Positional Embedding and Masking"
      ],
      "metadata": {
        "id": "4dDhatA3sWIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code Reference: https://github.com/whaleloops/TransformEHR/blob/main/icdmodelbart.py\n",
        "\n",
        "def create_position_ids_from_input_ids(input_ids, padding_idx):\n",
        "    \"\"\" Replace non-padding symbols with their position numbers. Position numbers begin at\n",
        "    padding_idx+1. Padding symbols are ignored. This is modified from fairseq's\n",
        "    `utils.make_positions`.\n",
        "    :param torch.Tensor x:\n",
        "    :return torch.Tensor:\n",
        "    \"\"\"\n",
        "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
        "    mask = input_ids.ne(padding_idx).int()\n",
        "    incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
        "    return incremental_indices.long() + padding_idx\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def invert_mask(attention_mask):\n",
        "    assert attention_mask.dim() == 2\n",
        "    return attention_mask.eq(0)\n",
        "\n",
        "\n",
        "def _make_linear_from_emb(emb):\n",
        "    vocab_size, emb_size = emb.weight.shape\n",
        "    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n",
        "    lin_layer.weight.data = emb.weight.data\n",
        "    return lin_layer\n",
        "\n",
        "\n",
        "# Helper Functions, mostly for making masks\n",
        "def _check_shapes(shape_1, shape2):\n",
        "    if shape_1 != shape2:\n",
        "        raise AssertionError(\"shape mismatch: {} != {}\".format(shape_1, shape2))\n",
        "\n",
        "\n",
        "def shift_tokens_right(input_ids, pad_token_id):\n",
        "    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n",
        "    prev_output_tokens = input_ids.clone()\n",
        "    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
        "    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
        "    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
        "    return prev_output_tokens\n",
        "\n",
        "\n",
        "def make_padding_mask(input_ids, padding_idx=1):\n",
        "    \"\"\"True for pad tokens\"\"\"\n",
        "    padding_mask = input_ids.eq(padding_idx)\n",
        "    if not padding_mask.any():\n",
        "        padding_mask = None\n",
        "    return padding_mask\n",
        "\n",
        "class DateYearMonthDayEmbedding(nn.Embedding):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings(year) and learned positional embedding(month day)\"\"\"\n",
        "    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n",
        "        # print(\"num_positions:\",num_positions) # 1024\n",
        "        # print(\"embedding_dim:\",embedding_dim)\n",
        "        print(\"DateYearMonthDayEmbedding- starting init\")\n",
        "        self.embed_year = SinusoidalPositionalEmbedding(num_positions, embedding_dim, padding_idx=padding_idx)\n",
        "        print(\"DateYearMonthDayEmbedding- finish SinusoidalPositionalEmbedding call??\")\n",
        "        self.embed_month = nn.Embedding(13, embedding_dim)\n",
        "        self.embed_day = nn.Embedding(32, embedding_dim)\n",
        "        print(\"DateYearMonthDayEmbedding- finish init\")\n",
        "\n",
        "    def forward(self, input, use_cache=False):\n",
        "        assert type(input) == datetime.datetime\n",
        "        year = self.embed_year(input.year)\n",
        "        month = self.embed_month(input.month)\n",
        "        day = self.embed_day(input.day)\n",
        "        # print(year,month,day)\n",
        "        return year + month + day\n",
        "\n",
        "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
        "\n",
        "    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n",
        "        print(\"SinusoidalPositionalEmbedding- starting init\")\n",
        "        super().__init__(num_positions, embedding_dim)\n",
        "        if embedding_dim % 2 != 0:\n",
        "            raise NotImplementedError(f\"odd embedding_dim {embedding_dim} not supported\")\n",
        "        self.weight = self._init_weight(self.weight)\n",
        "        print(\"SinusoidalPositionalEmbedding- finish init 1.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weight(out: nn.Parameter):\n",
        "        \"\"\"Identical to the XLM create_sinusoidal_embeddings except features are not interleaved.\n",
        "            The cos features are in the 2nd half of the vector. [dim // 2:]\n",
        "        \"\"\"\n",
        "        n_pos, dim = out.shape\n",
        "        position_enc = np.array(\n",
        "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
        "        )\n",
        "\n",
        "        # test\n",
        "        # out_copy = out.clone().detach().requires_grad_(False)\n",
        "        # sin_values = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
        "        # cos_values = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "        # out_copy[:, 0 : dim // 2].copy_(sin_values)\n",
        "        # out_copy[:, dim // 2 :].copy_(cos_values)\n",
        "        # out.detach_()\n",
        "        # out.requires_grad = False\n",
        "        # return out_copy\n",
        "\n",
        "        # test 2\n",
        "        sin_values = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
        "        cos_values = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "        # out[:, 0 : dim // 2].copy_(sin_values)\n",
        "        # out[:, dim // 2 :].copy_(cos_values)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          out[:, 0 : dim // 2].copy_(sin_values)\n",
        "          out[:, dim // 2 :].copy_(cos_values)\n",
        "\n",
        "        out.detach_()\n",
        "        out.requires_grad = False\n",
        "        print(\"SinusoidalPositionalEmbedding- finish init_weight 2.\")\n",
        "        return out\n",
        "\n",
        "        # # Orginial\n",
        "        # out[:, 0 : dim // 2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))  # This line breaks for odd n_pos\n",
        "        # out[:, dim // 2 :] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "        # out.detach_()\n",
        "        # out.requires_grad = False\n",
        "        # return out\n",
        "\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, input_ids, use_cache=False):\n",
        "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
        "        bsz, seq_len = input_ids.shape[:2]\n",
        "        if use_cache:\n",
        "            positions = input_ids.data.new(1, 1).fill_(seq_len - 1)  # called before slicing\n",
        "        else:\n",
        "            # starts at 0, ends at 1-seq_len\n",
        "            positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n",
        "        return super().forward(positions)\n",
        "\n",
        "class LearnedPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    This module learns positional embeddings up to a fixed maximum size.\n",
        "    Padding ids are ignored by either offsetting based on padding_idx\n",
        "    or by setting padding_idx to None and ensuring that the appropriate\n",
        "    position ids are passed to the forward function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_embeddings: int, embedding_dim: int, padding_idx: int,\n",
        "    ):\n",
        "        # if padding_idx is specified then offset the embedding ids by\n",
        "        # this index and adjust num_embeddings appropriately\n",
        "        print(\"LearnedPositionalEmbedding- starting init\")\n",
        "        assert padding_idx is not None\n",
        "        num_embeddings += padding_idx + 1  # WHY?\n",
        "        super().__init__(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "    def forward(self, input, use_cache=False):\n",
        "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
        "        if use_cache:  # the position is our current step in the decoded sequence\n",
        "            pos = int(self.padding_idx + input.size(1))\n",
        "            positions = input.data.new(1, 1).fill_(pos)\n",
        "        else:\n",
        "            positions = create_position_ids_from_input_ids(input, self.padding_idx)\n",
        "        return super().forward(positions)\n",
        "\n",
        "# Helper Modules\n",
        "\n",
        "def fill_with_neg_inf(t):\n",
        "    \"\"\"FP16-compatible function that fills a input_ids with -inf.\"\"\"\n",
        "    return t.float().fill_(float(\"-inf\")).type_as(t)\n",
        "\n",
        "\n",
        "def _filter_out_falsey_values(tup) -> Tuple:\n",
        "    \"\"\"Remove entries that are None or [] from an iterable.\"\"\"\n",
        "    return tuple(x for x in tup if isinstance(x, torch.Tensor) or x)\n",
        "\n",
        "\n",
        "# Public API\n",
        "def _get_shape(t):\n",
        "    return getattr(t, \"shape\", None)"
      ],
      "metadata": {
        "id": "_4nD3_55scjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Attention Mechanism"
      ],
      "metadata": {
        "id": "dElyDTrbsjgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code Reference: https://github.com/whaleloops/TransformEHR/blob/main/icdmodelbart.py\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        bias=True,\n",
        "        encoder_decoder_attention=False,  # otherwise self_attention\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.cache_key = \"encoder_decoder\" if self.encoder_decoder_attention else \"self\"\n",
        "\n",
        "    def _shape(self, tensor, dim_0, bsz):\n",
        "        return tensor.contiguous().view(dim_0, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query,\n",
        "        key: Optional[Tensor],\n",
        "        key_padding_mask: Optional[Tensor] = None,\n",
        "        layer_state: Optional[Dict[str, Optional[Tensor]]] = None,\n",
        "        attn_mask: Optional[Tensor] = None,\n",
        "        need_weights=False,\n",
        "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "        \"\"\"Input shape: Time(SeqLen) x Batch x Channel\"\"\"\n",
        "        static_kv: bool = self.encoder_decoder_attention\n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        # get here for encoder decoder cause of static_kv\n",
        "        if layer_state is not None:  # reuse k,v and encoder_padding_mask\n",
        "            saved_state = layer_state.get(self.cache_key, {})\n",
        "            if \"prev_key\" in saved_state:\n",
        "                # previous time steps are cached - no need to recompute key and value if they are static\n",
        "                if static_kv:\n",
        "                    key = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "            layer_state = {}\n",
        "\n",
        "        q = self.q_proj(query) * self.scaling\n",
        "        if static_kv:\n",
        "            if key is None:\n",
        "                k = v = None\n",
        "            else:\n",
        "                k = self.k_proj(key)\n",
        "                v = self.v_proj(key)\n",
        "        else:\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "\n",
        "        q = self._shape(q, tgt_len, bsz)\n",
        "        if k is not None:\n",
        "            k = self._shape(k, -1, bsz)\n",
        "        if v is not None:\n",
        "            v = self._shape(v, -1, bsz)\n",
        "\n",
        "        if saved_state is not None:\n",
        "            k, v, key_padding_mask = self._use_saved_state(k, v, saved_state, key_padding_mask, static_kv, bsz)\n",
        "\n",
        "        # Update cache\n",
        "        layer_state[self.cache_key] = {\n",
        "            \"prev_key\": k.view(bsz, self.num_heads, -1, self.head_dim),\n",
        "            \"prev_value\": v.view(bsz, self.num_heads, -1, self.head_dim),\n",
        "            \"prev_key_padding_mask\": key_padding_mask if not static_kv else None,\n",
        "        }\n",
        "\n",
        "        assert k is not None\n",
        "        src_len = k.size(1)\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        assert attn_weights.size() == (bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attn_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n",
        "            key_padding_mask = None\n",
        "        assert key_padding_mask is None or key_padding_mask.size()[:2] == (bsz, src_len,)\n",
        "\n",
        "        if key_padding_mask is not None:  # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            reshaped = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn_weights = attn_weights.masked_fill(reshaped, float(\"-inf\"))\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training,)\n",
        "\n",
        "        assert v is not None\n",
        "        attn_output = torch.bmm(attn_probs, v)\n",
        "        assert attn_output.size() == (bsz * self.num_heads, tgt_len, self.head_dim)\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights = None\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "    def _use_saved_state(self, k, v, saved_state, key_padding_mask, static_kv, bsz):\n",
        "        # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "        if \"prev_key\" in saved_state:\n",
        "            _prev_key = saved_state[\"prev_key\"]\n",
        "            assert _prev_key is not None\n",
        "            prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "            if static_kv:\n",
        "                k = prev_key\n",
        "            else:\n",
        "                assert k is not None\n",
        "                k = torch.cat([prev_key, k], dim=1)\n",
        "        if \"prev_value\" in saved_state:\n",
        "            _prev_value = saved_state[\"prev_value\"]\n",
        "            assert _prev_value is not None\n",
        "            prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "            if static_kv:\n",
        "                v = prev_value\n",
        "            else:\n",
        "                assert v is not None\n",
        "                v = torch.cat([prev_value, v], dim=1)\n",
        "        assert k is not None and v is not None\n",
        "        prev_key_padding_mask: Optional[Tensor] = saved_state.get(\"prev_key_padding_mask\", None)\n",
        "        key_padding_mask = self._cat_prev_key_padding_mask(\n",
        "            key_padding_mask, prev_key_padding_mask, bsz, k.size(1), static_kv\n",
        "        )\n",
        "        return k, v, key_padding_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def _cat_prev_key_padding_mask(\n",
        "        key_padding_mask: Optional[Tensor],\n",
        "        prev_key_padding_mask: Optional[Tensor],\n",
        "        batch_size: int,\n",
        "        src_len: int,\n",
        "        static_kv: bool,\n",
        "    ) -> Optional[Tensor]:\n",
        "        # saved key padding masks have shape (bsz, seq_len)\n",
        "        if prev_key_padding_mask is not None:\n",
        "            if static_kv:\n",
        "                new_key_padding_mask = prev_key_padding_mask\n",
        "            else:\n",
        "                new_key_padding_mask = torch.cat([prev_key_padding_mask, key_padding_mask], dim=1)\n",
        "\n",
        "        elif key_padding_mask is not None:\n",
        "            filler = torch.zeros(\n",
        "                batch_size,\n",
        "                src_len - key_padding_mask.size(1),\n",
        "                dtype=key_padding_mask.dtype,\n",
        "                device=key_padding_mask.device,\n",
        "            )\n",
        "            new_key_padding_mask = torch.cat([filler, key_padding_mask], dim=1)\n",
        "        else:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        return new_key_padding_mask\n"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Encoder and BART Encoder"
      ],
      "metadata": {
        "id": "yVH_F-HOsr9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code Reference: https://github.com/whaleloops/TransformEHR/blob/main/icdmodelbart.py\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.self_attn = SelfAttention(\n",
        "            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.normalize_before = config.normalize_before\n",
        "        self.self_attn_layer_norm = torch.nn.LayerNorm(self.embed_dim, 1e-5, True)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = torch.nn.LayerNorm(self.embed_dim, 1e-5, True)\n",
        "\n",
        "    def forward(self, x, encoder_padding_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
        "                `(batch, src_len)` where padding elements are indicated by ``1``.\n",
        "            for t_tgt, t_src is excluded (or masked out), =0 means it is\n",
        "            included in attention\n",
        "\n",
        "        Returns:\n",
        "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        # test\n",
        "        # if self.normalize_before:\n",
        "        #     x = self.self_attn_layer_norm(x)\n",
        "        x, attn_weights = self.self_attn(\n",
        "            query=x, key=x, key_padding_mask=encoder_padding_mask, need_weights=self.output_attentions\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        residual = x\n",
        "        # test\n",
        "        # if self.normalize_before:\n",
        "        #     x = self.final_layer_norm(x)\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "class BartEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n",
        "    is a :class:`EncoderLayer`.\n",
        "\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "\n",
        "        embed_dim = embed_tokens.embedding_dim\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "\n",
        "        self.embed_tokens = embed_tokens\n",
        "        # # test\n",
        "        # self.embed_positions = LearnedPositionalEmbedding(\n",
        "        #         config.max_position_embeddings, embed_dim, self.padding_idx,\n",
        "        #     )\n",
        "        # self.embed_visitids = nn.Embedding(1460, embed_dim)\n",
        "        if config.static_position_embeddings:\n",
        "            self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.max_position_embeddings, embed_dim, self.padding_idx\n",
        "            )\n",
        "        else:\n",
        "            self.embed_positions = LearnedPositionalEmbedding(\n",
        "                config.max_position_embeddings, embed_dim, self.padding_idx,\n",
        "            )\n",
        "        if config.date_visit_embeddings:\n",
        "            self.embed_visitids = DateYearMonthDayEmbedding(config.max_position_embeddings, embed_dim, self.padding_idx)\n",
        "        else:\n",
        "            self.embed_visitids = nn.Embedding(1460, embed_dim)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.layernorm_embedding = torch.nn.LayerNorm(embed_dim, 1e-5, True) if config.normalize_embedding else nn.Identity()\n",
        "        # mbart has one extra layer_norm\n",
        "        self.layer_norm = torch.nn.LayerNorm(config.d_model, 1e-5, True) if config.normalize_before else None\n",
        "        print(\"BartEncoder- finish init\")\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, attention_mask=None, visit_ids=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids (LongTensor): tokens in the source language of shape\n",
        "                `(batch, src_len)`\n",
        "            attention_mask (torch.LongTensor): indicating which indices are padding tokens.\n",
        "        Returns:\n",
        "            Tuple comprised of:\n",
        "                - **x** (Tensor): the last encoder layer's output of\n",
        "                  shape `(src_len, batch, embed_dim)`\n",
        "                - **encoder_states** (List[Tensor]): all intermediate\n",
        "                  hidden states of shape `(src_len, batch, embed_dim)`.\n",
        "                  Only populated if *self.output_hidden_states:* is True.\n",
        "                - **all_attentions** (List[Tensor]): Attention weights for each layer.\n",
        "                During training might not be of length n_layers because of layer dropout.\n",
        "        \"\"\"\n",
        "        # check attention mask and invert\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = invert_mask(attention_mask)\n",
        "\n",
        "        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "        embed_pos = self.embed_positions(input_ids)\n",
        "        embed_visit = self.embed_visitids(visit_ids)\n",
        "        x = inputs_embeds + embed_pos + embed_visit\n",
        "        x = self.layernorm_embedding(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        encoder_states, all_attentions = [], []\n",
        "        for encoder_layer in self.layers:\n",
        "            if self.output_hidden_states:\n",
        "                encoder_states.append(x)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                attn = None\n",
        "            else:\n",
        "                x, attn = encoder_layer(x, attention_mask)\n",
        "\n",
        "            if self.output_attentions:\n",
        "                all_attentions.append(attn)\n",
        "\n",
        "        if self.layer_norm:\n",
        "            x = self.layer_norm(x)\n",
        "        if self.output_hidden_states:\n",
        "            encoder_states.append(x)\n",
        "\n",
        "        # T x B x C -> B x T x C\n",
        "        encoder_states = [hidden_state.transpose(0, 1) for hidden_state in encoder_states]\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        return x, encoder_states, all_attentions"
      ],
      "metadata": {
        "id": "Gj4Bp2Nvsx9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decoder and BART Decoder"
      ],
      "metadata": {
        "id": "JcJOD4bGszX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code Reference: https://github.com/whaleloops/TransformEHR/blob/main/icdmodelbart.py\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.self_attn = SelfAttention(\n",
        "            embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.normalize_before = config.normalize_before\n",
        "\n",
        "        self.self_attn_layer_norm = torch.nn.LayerNorm(self.embed_dim, 1e-5, True)\n",
        "        self.encoder_attn = SelfAttention(\n",
        "            self.embed_dim,\n",
        "            config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            encoder_decoder_attention=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = torch.nn.LayerNorm(self.embed_dim, 1e-5, True)\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = torch.nn.LayerNorm(self.embed_dim, 1e-5, True)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attn_mask=None,\n",
        "        layer_state=None,\n",
        "        causal_mask=None,\n",
        "        decoder_padding_mask=None,\n",
        "    ):\n",
        "        residual = x\n",
        "\n",
        "        if layer_state is None:\n",
        "            layer_state = {}\n",
        "        if self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "        # Self Attention\n",
        "\n",
        "        x, self_attn_weights = self.self_attn(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            layer_state=layer_state,  # adds keys to layer state\n",
        "            key_padding_mask=decoder_padding_mask,\n",
        "            attn_mask=causal_mask,\n",
        "            need_weights=self.output_attentions,\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        # Cross attention\n",
        "        residual = x\n",
        "        assert self.encoder_attn.cache_key != self.self_attn.cache_key\n",
        "        if self.normalize_before:\n",
        "            x = self.encoder_attn_layer_norm(x)\n",
        "        x, _ = self.encoder_attn(\n",
        "            query=x,\n",
        "            key=encoder_hidden_states,\n",
        "            key_padding_mask=encoder_attn_mask,\n",
        "            layer_state=layer_state,  # mutates layer state\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.encoder_attn_layer_norm(x)\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        if not self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return (\n",
        "            x,\n",
        "            self_attn_weights,\n",
        "            layer_state,\n",
        "        )  # just self_attn weights for now, following t5, layer_state = cache for decoding\n",
        "\n",
        "\n",
        "class BartDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer\n",
        "    is a :class:`DecoderLayer`.\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens: nn.Embedding):\n",
        "        super().__init__()\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.decoder_layerdrop\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "        self.embed_tokens = embed_tokens\n",
        "        if config.static_position_embeddings:\n",
        "            self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.d_model, config.pad_token_id\n",
        "            )\n",
        "        else:\n",
        "            self.embed_positions = LearnedPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.d_model, self.padding_idx,\n",
        "            )\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderLayer(config) for _ in range(config.decoder_layers)]\n",
        "        )  # type: List[DecoderLayer]\n",
        "        self.layernorm_embedding = torch.nn.LayerNorm(config.d_model, 1e-5, True) if config.normalize_embedding else nn.Identity()\n",
        "        self.layer_norm = torch.nn.LayerNorm(config.d_model, 1e-5, True) if config.add_final_layer_norm else None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        encoder_hidden_states,\n",
        "        encoder_padding_mask,\n",
        "        decoder_padding_mask,\n",
        "        decoder_causal_mask,\n",
        "        decoder_cached_states=None,\n",
        "        use_cache=False,\n",
        "        **unused\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Includes several features from \"Jointly Learning to Align and\n",
        "        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n",
        "\n",
        "        Args:\n",
        "            input_ids (LongTensor): previous decoder outputs of shape\n",
        "                `(batch, tgt_len)`, for teacher forcing\n",
        "            encoder_hidden_states: output from the encoder, used for\n",
        "                encoder-side attention\n",
        "            encoder_padding_mask: for ignoring pad tokens\n",
        "            decoder_cached_states (dict or None): dictionary used for storing state during generation\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n",
        "                - hidden states\n",
        "                - attentions\n",
        "        \"\"\"\n",
        "        # check attention mask and invert\n",
        "        if encoder_padding_mask is not None:\n",
        "            encoder_padding_mask = invert_mask(encoder_padding_mask)\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_ids, use_cache=use_cache)\n",
        "\n",
        "        if use_cache:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "            positions = positions[:, -1:]  # happens after we embed them\n",
        "            # assert input_ids.ne(self.padding_idx).any()\n",
        "\n",
        "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
        "        x += positions\n",
        "        x = self.layernorm_embedding(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Convert to Bart output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n",
        "        x = x.transpose(0, 1)\n",
        "        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = ()\n",
        "        all_self_attns = ()\n",
        "        next_decoder_cache = []\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if self.output_hidden_states:\n",
        "                all_hidden_states += (x,)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):\n",
        "                continue\n",
        "\n",
        "            layer_state = decoder_cached_states[idx] if decoder_cached_states is not None else None\n",
        "\n",
        "            x, layer_self_attn, layer_past = decoder_layer(\n",
        "                x,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attn_mask=encoder_padding_mask,\n",
        "                decoder_padding_mask=decoder_padding_mask,\n",
        "                layer_state=layer_state,\n",
        "                causal_mask=decoder_causal_mask,\n",
        "            )\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache.append(layer_past.copy())\n",
        "\n",
        "            if self.layer_norm and (idx == len(self.layers) - 1):  # last layer of mbart\n",
        "                x = self.layer_norm(x)\n",
        "            if self.output_attentions:\n",
        "                all_self_attns += (layer_self_attn,)\n",
        "\n",
        "        # Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n",
        "        all_hidden_states = [hidden_state.transpose(0, 1) for hidden_state in all_hidden_states]\n",
        "        x = x.transpose(0, 1)\n",
        "        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n",
        "\n",
        "        if use_cache:\n",
        "            next_cache = ((encoder_hidden_states, encoder_padding_mask), next_decoder_cache)\n",
        "        else:\n",
        "            next_cache = None\n",
        "        return x, next_cache, all_hidden_states, list(all_self_attns)\n"
      ],
      "metadata": {
        "id": "8nL9UBnWs2t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Model1 - Bart (Bidirectional encoder and left-to-right decoder) Model"
      ],
      "metadata": {
        "id": "AS2EnuJCuc57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code Reference: https://github.com/whaleloops/TransformEHR/blob/main/icdmodelbart.py\n",
        "\n",
        "def _prepare_bart_decoder_inputs(\n",
        "    config, input_ids, decoder_input_ids=None, decoder_padding_mask=None, causal_mask_dtype=torch.float32\n",
        "):\n",
        "    \"\"\"Prepare masks that ignore padding tokens in the decoder and a causal mask for the decoder if\n",
        "    none are provided. This mimics the default behavior in fairseq. To override it pass in masks.\n",
        "    Note: this is not called during generation\n",
        "    \"\"\"\n",
        "    pad_token_id = config.pad_token_id\n",
        "    if decoder_input_ids is None:\n",
        "        decoder_input_ids = shift_tokens_right(input_ids, pad_token_id)\n",
        "    bsz, tgt_len = decoder_input_ids.size()\n",
        "    if decoder_padding_mask is None:\n",
        "        decoder_padding_mask = make_padding_mask(decoder_input_ids, pad_token_id)\n",
        "    else:\n",
        "        decoder_padding_mask = invert_mask(decoder_padding_mask)\n",
        "    causal_mask = torch.triu(fill_with_neg_inf(torch.zeros(tgt_len, tgt_len)), 1).to(\n",
        "        dtype=causal_mask_dtype, device=decoder_input_ids.device\n",
        "    )\n",
        "    return decoder_input_ids, decoder_padding_mask, causal_mask\n",
        "\n",
        "\n",
        "class PretrainedBartModel(PreTrainedModel):\n",
        "    config_class = BartConfig\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, SinusoidalPositionalEmbedding):\n",
        "            pass\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        pad_token = self.config.pad_token_id\n",
        "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
        "        dummy_inputs = {\n",
        "            \"attention_mask\": input_ids.ne(pad_token),\n",
        "            \"input_ids\": input_ids,\n",
        "        }\n",
        "        return dummy_inputs\n",
        "\n",
        "class BartModel(PretrainedBartModel):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx=padding_idx)\n",
        "        print(\"BartModel starting BartEncoder\")\n",
        "        self.encoder = BartEncoder(config, self.shared)\n",
        "        print(\"BartModel finish BartEncoder\")\n",
        "        self.decoder = BartDecoder(config, self.shared)\n",
        "        print(\"BartModel finish BartDecoder\")\n",
        "\n",
        "        self.init_weights()\n",
        "        print(\"BartModel finish init\")\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        encoder_outputs: Optional[Tuple] = None,\n",
        "        decoder_attention_mask=None,\n",
        "        decoder_cached_states=None,\n",
        "        use_cache=False,\n",
        "        visit_ids=None\n",
        "    ):\n",
        "\n",
        "        # make masks if user doesn't supply\n",
        "        if not use_cache:\n",
        "            decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n",
        "                self.config,\n",
        "                input_ids,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                decoder_padding_mask=decoder_attention_mask,\n",
        "                causal_mask_dtype=self.shared.weight.dtype,\n",
        "            )\n",
        "        else:\n",
        "            decoder_padding_mask, causal_mask = None, None\n",
        "\n",
        "        assert decoder_input_ids is not None\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, visit_ids=visit_ids)\n",
        "        assert isinstance(encoder_outputs, tuple)\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            decoder_input_ids,\n",
        "            encoder_outputs[0],\n",
        "            attention_mask,\n",
        "            decoder_padding_mask,\n",
        "            decoder_causal_mask=causal_mask,\n",
        "            decoder_cached_states=decoder_cached_states,\n",
        "            use_cache=use_cache,\n",
        "        )\n",
        "        # Attention and hidden_states will be [] or None if they aren't needed\n",
        "        decoder_outputs: Tuple = _filter_out_falsey_values(decoder_outputs)\n",
        "        assert isinstance(decoder_outputs[0], torch.Tensor)\n",
        "        encoder_outputs: Tuple = _filter_out_falsey_values(encoder_outputs)\n",
        "        return decoder_outputs + encoder_outputs\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return _make_linear_from_emb(self.shared)  # make it on the fly\n"
      ],
      "metadata": {
        "id": "WBE3qLq1unRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model2 - ICDBART Model"
      ],
      "metadata": {
        "id": "rJ_0cx4BvNjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code Reference: https://github.com/whaleloops/TransformEHR/blob/main/icdmodelbart.py\n",
        "\n",
        "def _reorder_buffer(attn_cache, new_order):\n",
        "    for k, input_buffer_k in attn_cache.items():\n",
        "        if input_buffer_k is not None:\n",
        "            attn_cache[k] = input_buffer_k.index_select(0, new_order)\n",
        "    return attn_cache\n",
        "\n",
        "class ICDBartForPreTraining(PretrainedBartModel):\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def __init__(self, config: BartConfig):\n",
        "        print(\"ICDBartForPreTraining starting super init\")\n",
        "        super().__init__(config)\n",
        "        print(\"ICDBartForPreTraining finised super init\")\n",
        "        base_model = BartModel(config)\n",
        "        print(\"ICDBartForPreTraining finised base_model\")\n",
        "        self.model = base_model\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        old_num_tokens = self.model.shared.num_embeddings\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
        "        self.model.shared = new_embeddings\n",
        "        self._resize_final_logits_bias(new_num_tokens, old_num_tokens)\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int, old_num_tokens: int) -> None:\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        decoder_cached_states=None,\n",
        "        lm_labels=None,\n",
        "        use_cache=False,\n",
        "        visit_ids=None,\n",
        "        **unused\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        lm_labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):\n",
        "            Labels for computing the masked language modeling loss.\n",
        "            Indices should either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` docstring).\n",
        "            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens\n",
        "            with labels\n",
        "            in ``[0, ..., config.vocab_size]``.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:\n",
        "        masked_lm_loss (`optional`, returned when ``lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Masked language modeling loss.\n",
        "        prediction_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`)\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
        "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
        "\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "            # Mask filling only works for bart-large\n",
        "            from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "            tokenizer = BartTokenizer.from_pretrained('bart-large')\n",
        "            TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
        "            model = BartForConditionalGeneration.from_pretrained('bart-large')\n",
        "            input_ids = tokenizer.batch_encode_plus([TXT], return_tensors='pt')['input_ids']\n",
        "            logits = model(input_ids)[0]\n",
        "            masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
        "            probs = logits[0, masked_index].softmax(dim=0)\n",
        "            values, predictions = probs.topk(5)\n",
        "            tokenizer.decode(predictions).split()\n",
        "            # ['good', 'great', 'all', 'really', 'very']\n",
        "        \"\"\"\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            decoder_cached_states=decoder_cached_states,\n",
        "            use_cache=use_cache,\n",
        "            visit_ids=visit_ids,\n",
        "        )\n",
        "        lm_logits = F.linear(outputs[0], self.model.shared.weight, bias=self.final_logits_bias)\n",
        "        outputs = (lm_logits,) + outputs[1:]  # Add cache, hidden states and attention if they are here\n",
        "        if lm_labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index = -100)\n",
        "            # TODO(SS): do we need to ignore pad tokens in lm_labels?\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), lm_labels.view(-1))\n",
        "            outputs = (masked_lm_loss,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def prepare_inputs_for_generation(self, decoder_input_ids, past, attention_mask, use_cache, **kwargs):\n",
        "        assert past is not None, \"past has to be defined for encoder_outputs\"\n",
        "\n",
        "        # first step, decoder_cached_states are empty\n",
        "        if not past[1]:\n",
        "            encoder_outputs, decoder_cached_states = past, None\n",
        "        else:\n",
        "            encoder_outputs, decoder_cached_states = past\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"decoder_cached_states\": decoder_cached_states,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_logits_for_generation(self, logits, cur_len, max_length):\n",
        "        if cur_len == 1:\n",
        "            self._force_token_ids_generation(logits, self.config.bos_token_id)\n",
        "        if cur_len == max_length - 1 and self.config.eos_token_id is not None:\n",
        "            self._force_token_ids_generation(logits, self.config.eos_token_id)\n",
        "        return logits\n",
        "\n",
        "    def _force_token_ids_generation(self, scores, token_ids) -> None:\n",
        "        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0\"\"\"\n",
        "        if isinstance(token_ids, int):\n",
        "            token_ids = [token_ids]\n",
        "        all_but_token_ids_mask = torch.tensor(\n",
        "            [x for x in range(self.config.vocab_size) if x not in token_ids],\n",
        "            dtype=torch.long,\n",
        "            device=next(self.parameters()).device,\n",
        "        )\n",
        "        assert len(scores.shape) == 2, \"scores should be of rank 2 with shape: [batch_size, vocab_size]\"\n",
        "        scores[:, all_but_token_ids_mask] = -float(\"inf\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past, beam_idx):\n",
        "        ((enc_out, enc_mask), decoder_cached_states) = past\n",
        "        reordered_past = []\n",
        "        for layer_past in decoder_cached_states:\n",
        "            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n",
        "            layer_past_new = {\n",
        "                attn_key: _reorder_buffer(attn_cache, beam_idx) for attn_key, attn_cache in layer_past.items()\n",
        "            }\n",
        "            reordered_past.append(layer_past_new)\n",
        "\n",
        "        new_enc_out = enc_out if enc_out is None else enc_out.index_select(0, beam_idx)\n",
        "        new_enc_mask = enc_mask if enc_mask is None else enc_mask.index_select(0, beam_idx)\n",
        "\n",
        "        past = ((new_enc_out, new_enc_mask), reordered_past)\n",
        "        return past\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.model.encoder\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return _make_linear_from_emb(self.model.shared)  # make it on the fly\n"
      ],
      "metadata": {
        "id": "3_8Zs410vL2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "\n",
        "Following are the Computational requirements for the implementation:\n",
        "\n",
        "*   Operating systems:\n",
        "    *   GPU (usage may be needed)\n",
        "    *   Google colab environment\n",
        "\n",
        "*   Python 3.8.11 with libraries:\n",
        "    *   NumPy (currently tested on version 1.20.3)\n",
        "    *   PyTorch (currently tested on version 1.9.0+cu111)\n",
        "    *   Transformers (currently tested on version 4.16.2)\n",
        "    *   Accelerate (0.21.0)\n",
        "    *   tqdm==4.62.2\n",
        "    *   scikit-learn==0.24.2\n",
        "    *   Pyhealth (v1.1.6 release - latest version)"
      ],
      "metadata": {
        "id": "GOOr3Zhos-ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Initialization"
      ],
      "metadata": {
        "id": "wdznvrH4MC3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing Pretrained Bart Model\n",
        "#Note: Customized code for Project purpose\n",
        "\n",
        "model_name = \"facebook/bart-large\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "config = BartConfig.from_pretrained('facebook/bart-large')\n",
        "print(config)\n",
        "config.static_position_embeddings = False\n",
        "config.date_visit_embeddings = True\n",
        "\n",
        "model = PretrainedBartModel(config)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "A2O1IC3sMBh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c59bff-2bf8-4356-accd-85913cfcd754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "PretrainedBartModel()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Training Configuration\n",
        "#Note: Customized code for Project purpose\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
        "TEST_ID_STRS = [\n",
        "    'chronic_PTSD_0',\n",
        "    'type_2_diabtes_0',\n",
        "    'hyperlipidemia_0',\n",
        "    'loin_pain_0',\n",
        "    'low_back_pain_0',\n",
        "    'PTSD_0',\n",
        "    'obstructive_sleep_apnea_hypopnea_0',\n",
        "    'mental_depression_0',\n",
        "    'chronic_obstructive_airway_disease_0',\n",
        "    'sensorineural_hearing_loss_0',\n",
        "    'gastroesophagel_reflux_disease_without_esophagitis_0',\n",
        "    'gastroesophagel_reflux_disease_0',\n",
        "    'coronary_arteriosclerosis_0',\n",
        "    'arteriosclerotic_heart_disease_0',\n",
        "    'chronic_PTSD_3',\n",
        "    'type_2_diabtes_3',\n",
        "    'hyperlipidemia_3',\n",
        "    'loin_pain_3',\n",
        "    'low_back_pain_3',\n",
        "    'PTSD_3',\n",
        "    'obstructive_sleep_apnea_hypopnea_3',\n",
        "    'mental_depression_3',\n",
        "    'chronic_obstructive_airway_disease_3',\n",
        "    'sensorineural_hearing_loss_3',\n",
        "    'gastroesophagel_reflux_disease_without_esophagitis_3',\n",
        "    'gastroesophagel_reflux_disease_3',\n",
        "    'coronary_arteriosclerosis_3',\n",
        "    'arteriosclerotic_heart_disease_3',\n",
        "    'chronic_PTSD_6',\n",
        "    'type_2_diabtes_6',\n",
        "    'hyperlipidemia_6',\n",
        "    'loin_pain_6',\n",
        "    'low_back_pain_6',\n",
        "    'PTSD_6',\n",
        "    'obstructive_sleep_apnea_hypopnea_6',\n",
        "    'mental_depression_6',\n",
        "    'chronic_obstructive_airway_disease_6',\n",
        "    'sensorineural_hearing_loss_6',\n",
        "    'gastroesophagel_reflux_disease_without_esophagitis_6',\n",
        "    'gastroesophagel_reflux_disease_6',\n",
        "    'coronary_arteriosclerosis_6',\n",
        "    'arteriosclerotic_heart_disease_6',\n",
        "    'least_happen'\n",
        "]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./results_icd\",\n",
        "    num_train_epochs = 3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    warmup_steps = 500,\n",
        "    weight_decay = 0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        "  )\n",
        "print(training_args)\n",
        "trainer = Trainer(\n",
        "     model=model,\n",
        "     args=training_args,\n",
        "     train_dataset=train_dataset,\n",
        "     eval_dataset=test_dataset,\n",
        "     tokenizer=tokenizer,\n",
        "     data_collator=data_collator,\n",
        "#     compute_metrics=compute_metrics\n",
        "     #prediction_loss_only=False\n",
        "  )\n"
      ],
      "metadata": {
        "id": "b5uq_kmjXEzc",
        "outputId": "9f7d4ce7-5425-4c30-8698-48fe521c294f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./results_icd/runs/Apr14_23-53-32_2cc4a4efe8c9,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./results_icd,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./results_icd,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=500,\n",
            "weight_decay=0.01,\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Training\n",
        "#Note: Customized code for Project purpose\n",
        "#Imp note: Unable to train model due to data collation/tokenization issue.\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "AbaEk435tKgL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "1e1edce5-15e5-4b07-ce22-4c98735dbd50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "22",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-9012eaf55337>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Note: Customized code for Project purpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Imp note: Unable to train model due to data collation/tokenization issue.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1781\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2085\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2086\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 22"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Evaluation"
      ],
      "metadata": {
        "id": "CZT1uD7FfgNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "#Note: Customized code for Project purpose\n",
        "#Imp note: Unable to run evaluate as it is dependent on Model training.\n",
        "eval_output = trainer.evaluate()\n",
        "print(eval_output)"
      ],
      "metadata": {
        "id": "6wASSO6KyU5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "Project Draft achievements:\n",
        "\n",
        "*   Extracted MIMIC IV dataset from https://physionet.org/content/mimiciv/2.2\n",
        "*   Mounted the data on Google drive\n",
        "*   Performed data preprocessing required to run the hypothesis. (Most of the time spent on getting this huge dataset extracted and creating the required subset to run with existing computing resources).\n",
        "*   Extracted the relevant models required for hypothesis and ablation implementation from the Github code available. (The paper implements multiple usecases and selecting the necessary code was challenging).\n",
        "*   Updated the Model implementation code to run without compilation errors. (We had to customize the Model Initialization, Training and evaluation part as the existing paper uses multiple classes which cater to different usecases).\n",
        "*   Attempted training BART model but currently facing challenges in Model training due to data collation/tokenization issue. (The paper uses Transformers library and customized few classes for the implementation. During the DLH course assignments, we have not used this library therefore understanding the usage of transformers library and implementing the code parallelly is the biggest challenge we are facing currently.)\n",
        "\n",
        "Final Project Plan:\n",
        "*   Planning to train and evaluate BART and ICDBART models without errors.\n",
        "*   Planning to train and evaluate BART and ICDBART models with Ablations planned.\n",
        "*   Planning to run the comparisons between the models to prove the hypothesis.\n",
        "\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO for Final Project"
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO for Final Project"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO for Final Project"
      ],
      "metadata": {
        "id": "zZq3qYS33jdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "\n",
        "\n",
        "1. Citation to Original Paper: Yang, Z., Mitra, A., Liu, W. et al. TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records. Nat Commun 14, 7857 (2023). https://doi.org/10.1038/s41467-023-43715-z. 2023 Nov 29;14(1):7857. doi: 10.1038/s41467-023-43715-z. PMID: 38030638; PMCID: PMC10687211.\n",
        "2. https://www.nature.com/articles/s41467-023-43715-z\n",
        "3. https://physionet.org/content/mimiciv/2.2/icu/#files-panel\n",
        "4. https://github.com/whaleloops/TransformEHR\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inorder to unmount the drive\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "S1N84HfXX85d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}